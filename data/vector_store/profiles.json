[
  {
    "id": "alex-spangher",
    "text": "Alex Spangher (USC) \u2014 Research Summary\n\nAlex Spangher\u2019s work centers on computational journalism and language technologies for creative, decision-driven writing. While a PhD researcher at USC\u2019s Information Sciences Institute (ISI), he built large, practice-grounded datasets and models to study how journalists plan, source, update, and prioritize information\u2014and used those insights to design AI systems that better support newsroom workflows. A unifying theme is modeling planning and decision-making in communication, which he later formalized under the banner of \u201cemulation learning.\u201d\n\nCore research strands\n\n1) Editing & factual updating in news (\u201cNewsEdits\u201d).\nSpangher led NewsEdits, the first public-scale corpus of revision histories for news articles (1.2M articles, 4.6M versions across 22 outlets, 2006\u20132021). He introduced document-level edit actions (Addition, Deletion, Edit, Refactor) and tasks for predicting revision behavior; the work received an Outstanding Paper recognition at NAACL 2022. He extended this line with NewsEdits 2.0, an edit-intentions schema distinguishing factual vs. stylistic/narrative changes and showed how predicting \u201cfact updates\u201d can drive safer LLM abstention on soon-to-be-stale content.\n\n2) Sourcing & source recommendation.\nHe constructed a large annotated dataset of informational sources in newswriting and trained strong models for source detection, attribution, and source prediction\u2014a step toward \u201csource recommendation\u201d tools that promote diverse, original sourcing. Follow-on work analyzes mixtures of sources and functional \u201csource schemas\u201d in narratives; this strand has been profiled as part of tools for helping journalists find diverse sources.\n\n3) Planning in creative writing (journalists vs. LLMs).\nIn Do LLMs Plan Like Human Writers?, Spangher assembled a PressRelease corpus (\u2248650k news articles linked to \u2248250k press releases) to compare newsroom planning behaviors with LLM outputs, revealing systematic gaps in planning and coverage/challenge dynamics; the paper earned an EMNLP 2024 Outstanding Paper Award.\n\n4) Newsworthiness & agenda setting.\nHe modeled which public documents become news by linking San Francisco legislative proposals, local news articles, and hours of meeting video via probabilistic relational modeling, quantifying coverage rates and predictive factors. This work grounds \u201cstory discovery\u201d tools in civic processes.\n\n5) Editorial prioritization and presentation.\nA recent preprint introduces NewsHomepages, a dataset of thousands of news site homepages to study layout decisions as signals of editorial prioritization.\n\n6) Agents & end-to-end evaluation.\nSpangher co-authored WebDS, a benchmark of 870 real web-based data-science tasks across 29 sites, showing current web agents struggle with grounding and repetitive behaviors\u2014evidence for more process-aware reasoning systems.\n\nThrough-line & impact\n\nAcross these strands, Spangher blends corpus building with causal, decision-oriented questions about how writers plan, which sources they select, when facts change, and what gets surfaced to audiences. His USC period features collaborations with ISI, Stanford\u2019s Big Local News, and industry, public talks on planning in creative contexts, and multiple paper recognitions (NAACL 2022 Outstanding; EMNLP 2024 Outstanding). Collectively, the work advances newsroom-aligned AI\u2014systems that reason about planning, sourcing, and updating, rather than just text generation.\n\nSelected datasets & tools: NewsEdits / NewsEdits 2.0 (revision + intentions), PressRelease (PR\u2013news linking), Sources (detection/attribution/prediction), Newsworthiness (policy\u2013news\u2013video linkage), NewsHomepages (layout signals), and WebDS (agentic data-science workflows)."
  },
  {
    "id": "dan-ho",
    "text": "Biography\nDaniel E. Ho is the William Benjamin Scott and Luna M. Scott Professor of Law, Professor of Political Science, Professor of Computer Science (by courtesy), Senior Fellow at the Stanford Institute for Human-Centered Artificial Intelligence (HAI), Senior Fellow at the Stanford Institute for Economic Policy Research, and Director of the Regulation, Evaluation, and Governance Lab (RegLab).\n\nHo served on the National Artificial Intelligence Advisory Committee (NAIAC), advising the White House on AI policy, as Senior Advisor on Responsible AI at the U.S. Department of Labor, on the Committee on National Statistics (CNSTAT) of the National Academies of Science, Engineering, and Medicine, as a Public Member of the Administrative Conference of the United States (ACUS), and as Special Advisor to the ABA Task Force on Law and Artificial Intelligence. He is an elected member of the American Academy of Arts and Sciences.\n\nHis scholarship focuses on administrative law, regulatory policy, and antidiscrimination law. With the RegLab, his work has developed high-impact demonstration projects of data science and machine learning in public policy, through partnerships with a range of government agencies, including the Internal Revenue Service, the Treasury Department, the Environmental Protection Agency, the San Francisco City Attorney\u2019s Office, the Department of Labor, Santa Clara County, and Seattle and King County Public Health.\n\nHe received his J.D. from Yale Law School and Ph.D. from Harvard University and clerked for Judge Stephen F. Williams on the U.S. Court of Appeals, District of Columbia Circuit. He is the recipient of numerous awards, including the John Bingham Hurlbut Award for Excellence in Teaching at Stanford Law School, the Carole Hafner Award for the best paper at the International Conference on Artificial Intelligence and Law, the Best Empirical Paper Prize from the American Law and Economics Review, Best Paper Awards at the annual meeting of the Association for Computational Linguistics (ACL), the ACM Conference on Fairness, Accountability, and Transparency (FAccT), the AAAI/ACM Conference on AI, Ethics, and Society (AIES), the SafeBench First Prize in AI Safety, and the Warren Miller prize for the best paper published in Political Analysis.\n\nDaniel E. Ho (Stanford) \u2014 extended research summary\n\nOverview & roles. Daniel E. Ho is the William Benjamin Scott & Luna M. Scott Professor of Law at Stanford, also in Political Science and (by courtesy) Computer Science; he directs the Regulation, Evaluation, and Governance Lab (RegLab), is a Senior Fellow at HAI and SIEPR, serves on the National AI Advisory Committee, and is Senior Advisor on Responsible AI at the U.S. Department of Labor. His group partners with agencies to modernize governance using data science and machine learning.\n\nMajor strands of research\n\n1) AI, law, and public-sector modernization.\nHo studies how to evaluate and govern AI in high-stakes settings and how to make government more evidence-driven. Recent work includes frameworks for governing open foundation models (Science, 2024), audits/benchmarking of legal AI tools and \u201clegal hallucinations\u201d (J. Legal Analysis, 2024; JELS, 2025), and practitioner-focused assessments of federal AI implementation and leadership mandates. He also helped blueprint the National AI Research Resource and develops domain benchmarks (e.g., LegalBench).\n\n2) Algorithmic fairness, disparity estimation, and administrative data.\nA large line of work tackles fairness when sensitive attributes (e.g., race) aren\u2019t directly observed, the privacy\u2013bias trade-off in government settings, pipeline-aware fairness, and equity for tax-audit selection. Examples include forthcoming JASA work on \u201cEstimating Racial Disparities When Race is Not Observed,\u201d studies of data minimization and fairness at FAccT, and income-aware fairness for IRS models.\n\n3) Environmental enforcement and remote sensing.\nWith agencies, RegLab has built computer-vision and satellite-imagery systems to detect environmental violations (e.g., intensive livestock operations/CAFOs; Clean Water Act compliance), plus randomized \u201cnudge\u201d trials that improved pollution-reporting timeliness. This combines near-real-time monitoring with policy evaluation to raise compliance.\n\n4) Public health & social service delivery.\nDuring COVID-19, Ho\u2019s team co-ran stepped-wedge and RCTs on contact-tracing operations (including language-matching to improve equity), integrated social services with disease investigation, and evaluated door-to-door testing allocations. More broadly, the lab uses administrative data to study operational limits and design improvements with county health departments.\n\n5) Administrative law, adjudication, and bureaucracy.\nHo also publishes in core public-law topics: executive control and structure of agency adjudication, congressional intervention in veterans\u2019 appeals, and the internal allocation of government work (\u201cGoverning by Assignment\u201d). These studies connect institutional design to observable performance and equity.\n\n6) Evidence-based governance methods.\nA methodological through-line is making causal inference feasible inside government. Notable contributions include \u201cFeasible Policy Evaluation by Design\u201d (randomized synthetic stepped-wedge trials), randomized peer-review experiments in food-safety inspection, and earlier work on causal-inference tooling such as MatchIt and guidance for credible inference in empirical legal studies.\n\n7) Earlier work in politics, law & information.\nEarlier strands include natural-experiment studies of ballot order and voting, measures of media positions, and analyses of the \u201cmarketplace of ideas.\u201d These projects blend statistical innovation with questions in democratic governance.\n\nThe RegLab model\n\nRegLab operates as an \u201cimpact lab\u201d inside government\u2014standing up demonstration projects with partners (federal, state, local) to co-design ML systems, audits, datasets, and RCTs on real workflows (e.g., food safety, environmental compliance, tax audits, UI eligibility, legal modernization such as mapping racial covenants). The lab also issues public reports and policy briefs that inform rulemaking and AI oversight.\n\nIf you want, I can tailor this to the specific sub-areas you care about (e.g., fairness methods, environmental monitoring, or legal-AI evaluation) and pull representative papers with 2\u20133 sentence takeaways for each."
  },
  {
    "id": "diyi-yang",
    "text": "I am an assistant professor in the Computer Science Department at Stanford, affiliated with the Stanford NLP Group, Stanford HCI Group, Stanford AI Lab (SAIL), and Stanford Human-Centered Artificial Intelligence (HAI). I am interested in Socially Aware NLP, Large Language Models (LLMs) and Human-AI Interaction, with a focus on how LLMs can augment human capabilities across research, work and well-being. My research goal is to design human-centered AI systems that are not only technically capable, but also meaningfully connected to how people think, interact, and collaborate.\n\nProspective students and postdocs, please check out this page on how to get involved.\n\nRecent Projects\nGenerative Interaction: Generative UI, Generative User Modeling (GUM)\nHuman-Agent Collaboration:Co-Gym, Future of Work with AI Agents\nLLMs and Large Audio Models: DiVA, Talk Arena, CAVA\nSocial Skill Training via LLMs: AI Partner and AI Mentor (APAM), Rehearsal\nDialects and Low-resource Context: VALUE, Multi-VALUE, DADA\nAI, Culture, and Society: CoMPost, NormBank, CultureBank, Unintended Impact\nAbout Me\nAssistant Professor, Computer Science Department at Stanford, 2022.9 - Present\nAssistant Professor, School of Interactive Computing at Georgia Tech, 2019.8 - 2022.8\nPh.D., Language Technologies Institute at Carnegie Mellon University, 2013 - 2019\nB.S., ACM Honored Class at Shanghai Jiao Tong University, 2009 - 2013\n\n\nSnapshot & Focus\n\nDiyi Yang is an Assistant Professor of Computer Science at Stanford University, affiliated with the Stanford NLP Group, HCI Group, SAIL, and HAI. Her lab develops socially aware, human-centered language technologies\u2014systems that can understand social context, support human communication, and behave responsibly across cultures and dialects. Her work bridges natural language processing (NLP), human-computer interaction (HCI), and computational social science.\n\n1. Human-Centered LLM Interaction & Collaboration\n\nA major strand of her research focuses on improving how humans interact and collaborate with large language models (LLMs).\n\nGenerative Interfaces (GenUI): Proposes systems that generate interactive user interfaces tailored to user goals, moving beyond static text responses.\n\nGeneral User Models (GUM): Builds dynamic user representations learned from real-world computer use (e.g., screenshots or activity traces) to enable proactive, context-aware assistants.\n\nAI and the Future of Work: Studies human-AI collaboration, introducing frameworks like the Human Agency Scale and the WORKBank dataset to measure how workers want to use AI for automation and augmentation.\n\n2. Speech & Audio Interaction with LLMs\n\nYang\u2019s group develops systems for voice-based collaboration between humans and AI.\n\nProjects like DiVA (Distilled Voice Assistant) and Talk Arena evaluate conversational agents through live, head-to-head speech interactions.\n\nThese works aim to close the gap between text-based and spoken dialogue systems, emphasizing conversational quality, adaptation, and user trust.\n\n3. Social Skill Training & Well-Being\n\nAnother thread explores LLMs as partners or mentors for social and emotional learning.\n\nSystems such as APAM (AI Partner & AI Mentor) and Rehearsal simulate interpersonal situations like conflict resolution or counseling training.\n\nThese models are tested in controlled studies to measure their effects on empathy, self-disclosure, and social skill development.\n\n4. Language Variation, Dialects & Low-Resource NLP\n\nYang has consistently worked on improving dialectal robustness and linguistic inclusivity in NLP.\n\nHer lab builds benchmarks like VALUE and Multi-VALUE for measuring model performance across English dialects.\n\nShe also introduces methods like DADA, which integrate linguistic rules to adapt models for low-resource dialects and domains.\n\n5. Culture, Norms & Societal Impacts of Alignment\n\nThis line investigates how LLMs encode and reflect cultural norms and moral values.\n\nDatasets such as NormBank and CultureBank collect situational norms and cultural values across societies.\n\nThe CoMPosT project analyzes how cultural caricature appears in AI simulations, while studies on alignment trace how model fine-tuning alters representations of dialect, ideology, and diversity.\n\n6. Online Communities, Social Roles & Supportive Interaction\n\nAn early and enduring theme in Yang\u2019s research examines social roles and supportive behavior in online communities.\n\nShe models interaction patterns in health forums, Wikipedia discussions, and crowdfunding sites, identifying key roles like seekers, providers, and moderators.\n\nThis work investigates how linguistic style and community structure affect self-disclosure, social support, and success in collaborative problem-solving.\n\n7. Methods, Datasets & Evaluation for NLP\n\nYang frequently pairs social goals with methodological advances.\n\nShe co-created ToTTo, a large benchmark for controlled table-to-text generation.\n\nOther projects span discourse-aware summarization, data augmentation techniques for low-data scenarios, and neural architectures such as hierarchical attention networks.\n\n8. Bias, Safety & Reasoning Behaviors in LLMs\n\nWith collaborators in HCI and responsible AI, she studies the sociotechnical effects of prompting and reasoning in LLMs.\n\nHer group shows that chain-of-thought prompting can sometimes amplify bias or toxicity in socially sensitive tasks.\n\nThey advocate for safer, more culturally grounded prompting strategies that align reasoning with human values.\n\nTeaching, Service & Roles\n\nAt Stanford, Yang teaches Human-Centered NLP, NLP for Computational Social Science, and CS224N. She has served on numerous ACL, EMNLP, CHI, and ICWSM committees and is Program Chair for ICLR 2026.\n\nThroughlines\n\nAcross all her strands, Yang\u2019s agenda integrates NLP, HCI, and social science to build AI systems that are:\n\nCollaborative \u2014 designed for meaningful human-AI interaction;\n\nEquitable \u2014 robust across dialects, cultures, and contexts; and\n\nResponsible \u2014 empirically grounded in social and ethical considerations.\n\nHer recent efforts\u2014GenUI, GUM, WORKBank, NormBank, and CultureBank\u2014reflect a cohesive research program toward practical, socially responsible AI that understands and supports human communication."
  },
  {
    "id": "sanmi-koyejo",
    "text": "Research\nI lead Stanford Trustworthy AI Research (STAIR). STAIR develops measurement-theoretic foundations for trustworthy AI systems, spanning AI evaluation science, algorithmic accountability, and privacy-preserving machine learning. Our applied research includes applications to healthcare, neuroimaging, and scientific discovery. For more details, see publications.\n\nSanmi Koyejo \u2014 extended research summary\n\nPositions & lab. Sanmi (Oluwasanmi) Koyejo is an Assistant Professor of Computer Science at Stanford University, where he leads the Stanford Trustworthy AI Research (STAIR) Lab. He is also an adjunct faculty member at the University of Illinois Urbana\u2013Champaign. His group develops measurement-theoretic foundations for trustworthy AI, spanning AI evaluation science, algorithmic accountability, and privacy-preserving learning, with applications in healthcare, neuroimaging, and scientific discovery.\n\nMajor strands of research\n\nEvaluation science & \u201cmeasurement for AI.\u201d\nKoyejo\u2019s recent agenda argues for moving beyond leaderboards toward a science of AI measurement\u2014making claims about AI systems precise, testable, and valid for the intended use. This includes a validity-centered evaluation framework and guidance for validating AI claims.\n\nReliability under distribution shift: fairness & robustness in the wild.\nA long-running thread studies when models fail to transfer fairness across settings (e.g., hospitals, devices, demographics) and how to diagnose the structure of shift to choose the right mitigations. Representative work: \u201cDiagnosing failures of fairness transfer across distribution shift in real-world medical settings\u201d (NeurIPS 2022).\n\nFederated, privacy-preserving, and adversary-robust learning.\nHis group develops methods and reviews for trustworthy distributed/federated AI: personalization with gradient privacy, vertical federated learning, and defenses against byzantine or majority-adversarial clients. Examples include robust vertical FL, byzantine-robust aggregation via clustering, and survey/tutorial papers on trustworthy distributed AI.\n\nFoundations & empirical scrutiny of LLM phenomena.\nKoyejo is co-author of \u201cAre Emergent Abilities of Large Language Models a Mirage?\u201d (NeurIPS 2023 oral), which shows many reported \u201cemergent\u201d jumps are artifacts of metric choice/statistics rather than sudden capability phase transitions\u2014reframing how progress should be measured.\n\nNeuroimaging & healthcare applications.\nEarlier and ongoing applied work includes fMRI decoding and structured probabilistic models, synthetic data augmentation for neuroimaging, and broader healthcare-AI collaborations\u2014mirroring the lab\u2019s emphasis on safety, equity, and robustness in clinical settings.\n\nCommunity leadership, service, and translation\n\nConference leadership. General Chair of NeurIPS 2022.\n\nInclusion & field-building. Co-founder and leader within Black in AI.\n\nEntrepreneurship & tech transfer. Founder/executive at Virtue AI, a startup focused on AI safety and security.\n\nTeaching & campus roles. At Stanford, he teaches courses such as CS329H (Machine Learning from Human Preferences) and CS221, and participates in initiatives on AI safety and responsible health AI.\n\nHow the strands fit together\n\nAcross these lines, the through-line is trustworthiness by design and by evidence: (i) formalizing what we claim an AI system does (evaluation science), (ii) designing learning algorithms that remain fair, private, and robust under realistic shifts (federated/robust ML), and (iii) stress-testing high-profile claims in modern AI (e.g., LLM \u201cemergence\u201d) with careful measurement. The applied work in healthcare and neuroimaging serves as a proving ground, where distribution shifts and equity stakes are concrete."
  }
]