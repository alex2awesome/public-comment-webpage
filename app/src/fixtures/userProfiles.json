[
  {
    "id": "alex-spangher",
    "name": "Alex Spangher",
    "role": "Postdoctoral scholar",
    "organization": "Stanford University",
    "homepage_url": "https://alexspangher.com",
    "interests": [
      "Computational Journalism",
      "Language Technologies",
      "Human-AI Collaboration"
    ],
    "last_updated": "2025-10-08",
    "summary": "Alex Spangher (USC) \u2014 Research Summary\n\nAlex Spangher\u2019s work centers on computational journalism and language technologies for creative, decision-driven writing. While a PhD researcher at USC\u2019s Information Sciences Institute (ISI), he built large, practice-grounded datasets and models to study how journalists plan, source, update, and prioritize information\u2014and used those insights to design AI systems that better support newsroom workflows. A unifying theme is modeling planning and decision-making in communication, which he later formalized under the banner of \u201cemulation learning.\u201d\n\nCore research strands\n\n1) Editing & factual updating in news (\u201cNewsEdits\u201d).\nSpangher led NewsEdits, the first public-scale corpus of revision histories for news articles (1.2M articles, 4.6M versions across 22 outlets, 2006\u20132021). He introduced document-level edit actions (Addition, Deletion, Edit, Refactor) and tasks for predicting revision behavior; the work received an Outstanding Paper recognition at NAACL 2022. He extended this line with NewsEdits 2.0, an edit-intentions schema distinguishing factual vs. stylistic/narrative changes and showed how predicting \u201cfact updates\u201d can drive safer LLM abstention on soon-to-be-stale content.\n\n2) Sourcing & source recommendation.\nHe constructed a large annotated dataset of informational sources in newswriting and trained strong models for source detection, attribution, and source prediction\u2014a step toward \u201csource recommendation\u201d tools that promote diverse, original sourcing. Follow-on work analyzes mixtures of sources and functional \u201csource schemas\u201d in narratives; this strand has been profiled as part of tools for helping journalists find diverse sources.\n\n3) Planning in creative writing (journalists vs. LLMs).\nIn Do LLMs Plan Like Human Writers?, Spangher assembled a PressRelease corpus (\u2248650k news articles linked to \u2248250k press releases) to compare newsroom planning behaviors with LLM outputs, revealing systematic gaps in planning and coverage/challenge dynamics; the paper earned an EMNLP 2024 Outstanding Paper Award.\n\n4) Newsworthiness & agenda setting.\nHe modeled which public documents become news by linking San Francisco legislative proposals, local news articles, and hours of meeting video via probabilistic relational modeling, quantifying coverage rates and predictive factors. This work grounds \u201cstory discovery\u201d tools in civic processes.\n\n5) Editorial prioritization and presentation.\nA recent preprint introduces NewsHomepages, a dataset of thousands of news site homepages to study layout decisions as signals of editorial prioritization.\n\n6) Agents & end-to-end evaluation.\nSpangher co-authored WebDS, a benchmark of 870 real web-based data-science tasks across 29 sites, showing current web agents struggle with grounding and repetitive behaviors\u2014evidence for more process-aware reasoning systems.\n\nThrough-line & impact\n\nAcross these strands, Spangher blends corpus building with causal, decision-oriented questions about how"
  },
  {
    "id": "dan-ho",
    "name": "Daniel E. Ho",
    "role": "Professor",
    "organization": "Stanford Law School",
    "homepage_url": "https://law.stanford.edu/daniel-e-ho/",
    "interests": [
      "Administrative Law",
      "Data Science",
      "Technology Policy"
    ],
    "last_updated": "2025-10-08",
    "summary": "Biography\nDaniel E. Ho is the William Benjamin Scott and Luna M. Scott Professor of Law, Professor of Political Science, Professor of Computer Science (by courtesy), Senior Fellow at the Stanford Institute for Human-Centered Artificial Intelligence (HAI), Senior Fellow at the Stanford Institute for Economic Policy Research, and Director of the Regulation, Evaluation, and Governance Lab (RegLab).\n\nHo served on the National Artificial Intelligence Advisory Committee (NAIAC), advising the White House on AI policy, as Senior Advisor on Responsible AI at the U.S. Department of Labor, on the Committee on National Statistics (CNSTAT) of the National Academies of Science, Engineering, and Medicine, as a Public Member of the Administrative Conference of the United States (ACUS), and as Special Advisor to the ABA Task Force on Law and Artificial Intelligence. He is an elected member of the American Academy of Arts and Sciences.\n\nHis scholarship focuses on administrative law, regulatory policy, and antidiscrimination law. With the RegLab, his work has developed high-impact demonstration projects of data science and machine learning in public policy, through partnerships with a range of government agencies, including the Internal Revenue Service, the Treasury Department, the Environmental Protection Agency, the San Francisco City Attorney\u2019s Office, the Department of Labor, Santa Clara County, and Seattle and King County Public Health.\n\nHe received his J.D. from Yale Law School and Ph.D. from Harvard University and clerked for Judge Stephen F. Williams on the U.S. Court of Appeals, District of Columbia Circuit. He is the recipient of numerous awards, including the John Bingham Hurlbut Award for Excellence in Teaching at Stanford Law School, the Carole Hafner Award for the best paper at the International Conference on Artificial Intelligence and Law, the Best Empirical Paper Prize from the American Law and Economics Review, Best Paper Awards at the annual meeting of the Association for Computational Linguistics (ACL), the ACM Conference on Fairness, Accountability, and Transparency (FAccT), the AAAI/ACM Conference on AI, Ethics, and Society (AIES), the SafeBench First Prize in AI Safety, and the Warren Miller prize for the best paper published in Political Analysis.\n\nDaniel E. Ho (Stanford) \u2014 extended research summary\n\nOverview & roles. Daniel E. Ho is the William Benjamin Scott & Luna M. Scott Professor of Law at Stanford, also in Political Science and (by courtesy) Computer Science; he directs the Regulation, Evaluation, and Governance Lab (RegLab), is a Senior Fellow at HAI and SIEPR, serves on the National AI Advisory Committee, and is Senior Advisor on Responsible AI at the U.S. Department of Labor. His group partners with agencies to modernize governance using data science and machine learning.\n\nMajor strands of research\n\n1) AI, law, and public-sector modernization.\nHo studies how to evaluate and govern AI in high-stakes settings and how to make governme"
  },
  {
    "id": "diyi-yang",
    "name": "Diyi Yang",
    "role": "Assistant Professor",
    "organization": "Stanford Computer Science",
    "homepage_url": "https://cs.stanford.edu/~diyiy/",
    "interests": [
      "Human-centered NLP",
      "Computational Social Science",
      "Responsible AI"
    ],
    "last_updated": "2025-10-08",
    "summary": "I am an assistant professor in the Computer Science Department at Stanford, affiliated with the Stanford NLP Group, Stanford HCI Group, Stanford AI Lab (SAIL), and Stanford Human-Centered Artificial Intelligence (HAI). I am interested in Socially Aware NLP, Large Language Models (LLMs) and Human-AI Interaction, with a focus on how LLMs can augment human capabilities across research, work and well-being. My research goal is to design human-centered AI systems that are not only technically capable, but also meaningfully connected to how people think, interact, and collaborate.\n\nProspective students and postdocs, please check out this page on how to get involved.\n\nRecent Projects\nGenerative Interaction: Generative UI, Generative User Modeling (GUM)\nHuman-Agent Collaboration:Co-Gym, Future of Work with AI Agents\nLLMs and Large Audio Models: DiVA, Talk Arena, CAVA\nSocial Skill Training via LLMs: AI Partner and AI Mentor (APAM), Rehearsal\nDialects and Low-resource Context: VALUE, Multi-VALUE, DADA\nAI, Culture, and Society: CoMPost, NormBank, CultureBank, Unintended Impact\nAbout Me\nAssistant Professor, Computer Science Department at Stanford, 2022.9 - Present\nAssistant Professor, School of Interactive Computing at Georgia Tech, 2019.8 - 2022.8\nPh.D., Language Technologies Institute at Carnegie Mellon University, 2013 - 2019\nB.S., ACM Honored Class at Shanghai Jiao Tong University, 2009 - 2013\n\n\nSnapshot & Focus\n\nDiyi Yang is an Assistant Professor of Computer Science at Stanford University, affiliated with the Stanford NLP Group, HCI Group, SAIL, and HAI. Her lab develops socially aware, human-centered language technologies\u2014systems that can understand social context, support human communication, and behave responsibly across cultures and dialects. Her work bridges natural language processing (NLP), human-computer interaction (HCI), and computational social science.\n\n1. Human-Centered LLM Interaction & Collaboration\n\nA major strand of her research focuses on improving how humans interact and collaborate with large language models (LLMs).\n\nGenerative Interfaces (GenUI): Proposes systems that generate interactive user interfaces tailored to user goals, moving beyond static text responses.\n\nGeneral User Models (GUM): Builds dynamic user representations learned from real-world computer use (e.g., screenshots or activity traces) to enable proactive, context-aware assistants.\n\nAI and the Future of Work: Studies human-AI collaboration, introducing frameworks like the Human Agency Scale and the WORKBank dataset to measure how workers want to use AI for automation and augmentation.\n\n2. Speech & Audio Interaction with LLMs\n\nYang\u2019s group develops systems for voice-based collaboration between humans and AI.\n\nProjects like DiVA (Distilled Voice Assistant) and Talk Arena evaluate conversational agents through live, head-to-head speech interactions.\n\nThese works aim to close the gap between text-based and spoken dialogue systems, emphasizing conversational quality, ada"
  },
  {
    "id": "sanmi-koyejo",
    "name": "Sanmi Koyejo",
    "role": "Associate Professor",
    "organization": "Stanford Computer Science",
    "homepage_url": "https://cs.stanford.edu/~sanmi/",
    "interests": [
      "Machine Learning",
      "Neuroscience",
      "Bayesian Methods"
    ],
    "last_updated": "2025-10-08",
    "summary": "Research\nI lead Stanford Trustworthy AI Research (STAIR). STAIR develops measurement-theoretic foundations for trustworthy AI systems, spanning AI evaluation science, algorithmic accountability, and privacy-preserving machine learning. Our applied research includes applications to healthcare, neuroimaging, and scientific discovery. For more details, see publications.\n\nSanmi Koyejo \u2014 extended research summary\n\nPositions & lab. Sanmi (Oluwasanmi) Koyejo is an Assistant Professor of Computer Science at Stanford University, where he leads the Stanford Trustworthy AI Research (STAIR) Lab. He is also an adjunct faculty member at the University of Illinois Urbana\u2013Champaign. His group develops measurement-theoretic foundations for trustworthy AI, spanning AI evaluation science, algorithmic accountability, and privacy-preserving learning, with applications in healthcare, neuroimaging, and scientific discovery.\n\nMajor strands of research\n\nEvaluation science & \u201cmeasurement for AI.\u201d\nKoyejo\u2019s recent agenda argues for moving beyond leaderboards toward a science of AI measurement\u2014making claims about AI systems precise, testable, and valid for the intended use. This includes a validity-centered evaluation framework and guidance for validating AI claims.\n\nReliability under distribution shift: fairness & robustness in the wild.\nA long-running thread studies when models fail to transfer fairness across settings (e.g., hospitals, devices, demographics) and how to diagnose the structure of shift to choose the right mitigations. Representative work: \u201cDiagnosing failures of fairness transfer across distribution shift in real-world medical settings\u201d (NeurIPS 2022).\n\nFederated, privacy-preserving, and adversary-robust learning.\nHis group develops methods and reviews for trustworthy distributed/federated AI: personalization with gradient privacy, vertical federated learning, and defenses against byzantine or majority-adversarial clients. Examples include robust vertical FL, byzantine-robust aggregation via clustering, and survey/tutorial papers on trustworthy distributed AI.\n\nFoundations & empirical scrutiny of LLM phenomena.\nKoyejo is co-author of \u201cAre Emergent Abilities of Large Language Models a Mirage?\u201d (NeurIPS 2023 oral), which shows many reported \u201cemergent\u201d jumps are artifacts of metric choice/statistics rather than sudden capability phase transitions\u2014reframing how progress should be measured.\n\nNeuroimaging & healthcare applications.\nEarlier and ongoing applied work includes fMRI decoding and structured probabilistic models, synthetic data augmentation for neuroimaging, and broader healthcare-AI collaborations\u2014mirroring the lab\u2019s emphasis on safety, equity, and robustness in clinical settings.\n\nCommunity leadership, service, and translation\n\nConference leadership. General Chair of NeurIPS 2022.\n\nInclusion & field-building. Co-founder and leader within Black in AI.\n\nEntrepreneurship & tech transfer. Founder/executive at Virtue AI, a startup focused on AI safety and "
  }
]