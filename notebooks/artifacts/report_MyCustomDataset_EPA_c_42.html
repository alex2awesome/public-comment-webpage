
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>C AutoMetric Report Card</title>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/css/bootstrap.min.css">
  <link rel="stylesheet" href="https://cdn.datatables.net/2.0.8/css/dataTables.dataTables.min.css">
  <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  <script src="https://cdn.plot.ly/plotly-2.27.0.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>
  
  <style>
    body.dark-mode { background-color: #121212; color: #e0e0e0; }
    body.dark-mode .card { background-color: #1e1e1e; border-color: #333; color: #e0e0e0; }
    body.dark-mode .table, body-dark-mode .table td { background-color: #1e1e1e; color: #e0e0e0; border-color: #333; }
  </style>
  <script>const RC_CORR = {}; const RC_RUNTIME = {}; const RC_ROB = {"available": false}; const RC_DOCS = {"Evidentiary_Support_and_Citations_Rubric": "---\n# Metric Card for Evidentiary_Support_and_Citations_Rubric\n\n**Evidentiary Support and Citations** Use of data, analyses, studies, and references that substantiate claims and quantify effects.\n\n## Metric Details\n\n**Evidentiary_Support_and_Citations_Rubric** is a **reference-free** LLM-as-a-Judge metric that prompts an LLM to rate a system output along a single, run-time-specified evaluation axis.\nIn this case the axis is `**Evidentiary Support and Citations** Use of data, analyses, studies, and references that substantiate claims and quantify effects.`.\n\nThe prompt supplies:\n\n1. **Task description** *d*\n2. **Rubric** `**Evidentiary Support and Citations** Use of data, analyses, studies, and references that substantiate claims and quantify effects.`\n3. **Input text** *x*\n4. **Output text** *y*\n\nGreedy decoding (temperature = 0) yields an integer score $\\hat{s}\\!\\in\\!\\{1,2,3,4,5\\}$; higher = better adherence to the axis.\n\n- **Metric Type:** LLM as a Judge\n- **Range:** 1-5 (1 = worst, 5 = best)\n- **Higher is Better?:** Yes\n- **Reference-Based?:** No\n- **Input-Required?:** Yes\n\n### Formal Definition\n\nLet $f _{\\theta}$ be the LLM and\n$\\pi _{\text{RF}}(d,\\{axis\\},x,y)$ construct the textual prompt.\n\n$$\n\\hat{s} \\;=\\; \\operatorname*{arg\\,max}\\limits_{s \\in \\{1,\\dots,5\\}} f _{\theta}\\!\bigl(s \\,\bigl|\\, \\pi _{\text{RF}}(d,\\{axis\\},x,y)\bigr)\n$$\n\nThe metric value is $\\operatorname{LJ}^{\text{RF}}_{\\{axis\\}}(d,x,y)=\\hat{s}$.\n\n### Rubric Details\n\n**Criteria:** **Evidentiary Support and Citations** Use of data, analyses, studies, and references that substantiate claims and quantify effects.\n\n#### Scoring Rubric\n\n| Score | Description |\n|-------|-------------|\n| 1 | Score 1: No evidentiary support<br/>\u2022 Purely opinion/anecdote; no citations or links.<br/>\u2022 No quantitative information supporting claims.<br/>\u2022 References are irrelevant, unverifiable, or obviously inaccurate.<br/>\u2022 \u201cSee attached\u201d or name-dropping without incorporating any evidence into the argument.<br/>\u2022 Fabricated-sounding or conflicting facts with no sources. |\n| 2 | Score 2: Minimal, vague, or weak support<br/>\u2022 One or two vague references (e.g., \u201cstudies show,\u201d \u201cEPA reports\u201d) without titles, dates, docket IDs, or URLs.<br/>\u2022 Mentions numbers but provides no source or traceability.<br/>\u2022 Sources are low-credibility (blogs, opinion pieces) or links are broken.<br/>\u2022 Evidence is not integrated into reasoning; limited or incorrect interpretation of cited material. |\n| 3 | Score 3: Adequate baseline support<br/>\u2022 At least one specific, credible citation (e.g., docket ID, Federal Register cite, named report/study with year).<br/>\u2022 Includes some quantitative data tied to a source.<br/>\u2022 Evidence generally supports key claims, though coverage may be partial or selective.<br/>\u2022 Minor gaps in traceability (missing page numbers, incomplete links) or reliance on secondary summaries.<br/>\u2022 Limited discussion of methods/limitations; little triangulation. |\n| 4 | Score 4: Strong, well-integrated support<br/>\u2022 Multiple credible, traceable sources (e.g., peer-reviewed studies, government reports, statutes/caselaw, docket materials) with precise identifiers (titles, dates, page/section numbers, URLs).<br/>\u2022 Quantifies effects clearly (units, magnitudes, time frames) and ties numbers to sources.<br/>\u2022 Evidence is accurately interpreted and woven into arguments that inform agency action.<br/>\u2022 Notes key assumptions/limitations; some triangulation across independent sources.<br/>\u2022 Attachments/exhibits referenced in-text; links work; minimal inconsistencies. |\n| 5 | Score 5: Exceptional, decision-grade evidence use<br/>\u2022 Comprehensive, diverse evidence base (primary studies, official data, legal/technical references) with precise, reproducible citations (page/figure numbers, dataset names, docket IDs).<br/>\u2022 Provides original analysis or calculations (methods shown, equations/assumptions stated), data tables/appendices, or replicable methodology.<br/>\u2022 Quantifies impacts rigorously (comparative baselines, confidence/uncertainty where applicable) and addresses counter-evidence.<br/>\u2022 Clearly links evidence to concrete recommendations/options, showing decision relevance.<br/>\u2022 All references are authoritative and accessible; no traceability gaps; consistent and accurate interpretation throughout. |\n\n### Inputs and Outputs\n- **Inputs:**\n  - **Task description** *d*\n  - **Rubric** `**Evidentiary Support and Citations** Use of data, analyses, studies, and references that substantiate claims and quantify effects.`\n  - **Input text** *x*\n  - **Output text** *y*\n- **Outputs:**\n  - Scalar score $\\hat{s} \\in \\{1,2,3,4,5\\}$\n\n## Intended Use\n\n- **Domain:** Policy feedback triage / Evidence quality evaluation\n- **Tasks:** \n  - Rank candidate drafts by strength of evidentiary support for escalation decisions\n  - Detect vague, missing, or fabricated citations and flag traceability issues\n  - Assess whether quantitative claims are sourced and whether magnitudes/timeframes are provided\n  - Extract cited sources (titles, years, docket IDs, URLs) and summarize their relevance\n  - Classify submissions into rubric categories (1\u20135) and provide brief rationale\n  - Suggest concrete citation improvements or additional evidence needed for escalation\n- **Best Suited For:** \n  - Large volumes of text-based policy feedback where fast, consistent triage is needed (e.g., initial pass to prioritize staff review).\n  - Drafts that include explicit citation text (titles, authors, docket IDs, URLs) so the model can evaluate specificity and traceability.\n  - Scenarios where the goal is to apply a fixed rubric across many items rather than perform final legal/technical adjudication.\n  - Environments where the model\u2019s output will be combined with human review or automated link-checking for verification.\n  - When submissions make quantitative claims that can be judged for presence/absence of sources and unit/magnitude clarity.\n  - Situations that benefit from consistent, descriptive feedback to contributors on how to improve evidence and citations.\n- **Not Recommended For:** \n  - Needs real-time verification of external links, paywalled or subscription-only documents, or live data lookups that the model cannot perform.\n  - Final agency decision-making or legal admissibility determinations that require authoritative primary-source validation.\n  - Highly technical scientific or engineering claims that require reanalysis of primary datasets or replication of methods.\n  - When submissions rely on unpublished/confidential documents or proprietary databases the model cannot access or verify.\n  - Languages, formats, or domain-specific citation conventions the model has not been trained on or that are highly specialized.\n  - Situations where hallucination risk is unacceptable and every citation must be independently confirmed without human oversight.\n\n## Metric Implementation\n\n### Reference Implementations\n\n- **Libraries/Packages:**\n  - [AutoMetrics LLM as a Judge (reference-free)](https://github.com/XenonMolecule/autometrics/blob/main/autometrics/metrics/generated/GeneratedLLMJudgeMetric.py)\n\n### Computational Complexity\n\n- **Efficiency:**\n  - Requires a single LLM call per input-output pair.\n  - AutoMetrics does parallel calls on batched inputs.\n\n- **Scalability:**\n  - Performance is linear in the number of input-output pairs.\n  - Performance depends on the underlying LLM model and the dataset size.  Additional consideration would include whether or not the LLM is a reasoning model.\n\n## Known Limitations\n\n- **Biases:** \n  - Authority bias: favoring government/academic sources and downgrading community, industry, or non\u2011traditional evidence regardless of relevance.\n  - Recency bias: giving more weight to newer sources even when older foundational studies or legal citations are more relevant.\n  - Format bias: preferring fully formatted citations and URLs over informal but accurate references or attachments.\n  - Language and geographic bias: privileging English\u2011language or U.S. federal sources and undervaluing non\u2011English or local/regional evidence.\n  - Quantitative bias: favoring submissions that present numbers even when qualitative evidence is more informative for the issue.\n  - Traceability bias: equating the presence of a URL or docket ID with credibility without verifying content.\n  - Publication bias: overvaluing peer\u2011reviewed literature and undervaluing credible gray literature (technical reports, agency memos).\n  - Confirmation bias: tending to credit evidence that aligns with common assumptions about policy impacts more readily than surprising counterevidence.\n  - Availability bias: relying on sources the model has been trained on or can recall easily, disadvantaging obscure but valid sources.\n  - Conservatism bias: penalizing novel analyses or original calculations because they lack an established publication trail.\n- **Task Misalignment Risks:** \n  - Scoring may prioritize traceable citation form over decision relevance, favoring citations that are easily verifiable rather than those most directly informing agency action.\n  - The rubric can encourage citation padding\u2014adding many superficial or low\u2011quality references to get a higher score\u2014rather than substantive evidence integration.\n  - It may penalize concise but high\u2011quality submissions that summarize evidence without reproducing full citations or links.\n  - The judge might undervalue first\u2011hand community testimony or qualitative harms that are highly relevant for policy but lack traditional citations.\n  - Because the model cannot access paywalled or proprietary attachments, it may downgrade otherwise strong, decision\u2011relevant evidence that isn't public.\n  - The rubric assumes public, reproducible citations; submissions based on internal agency data or confidential sources could be misjudged.\n  - Local or context\u2011specific evidence (e.g., tribal, municipal data) may be unfairly scored lower for not matching national\u2011level citation norms.\n  - The axis focuses on evidentiary depth and may miss timeliness or urgency signals that warrant escalation despite limited citations.\n- **Failure Cases:** \n  - Hallucinated verification: the model asserts that quoted citations are valid or finds supporting text when it cannot actually access the source.\n  - False negatives for shorthand citations: downgrading submissions that use accepted shorthand (e.g., 'EPA Toxics 2020') even though a human reviewer would recognize them.\n  - False positives for fabricated citations: awarding higher scores to convincing but nonexistent reports or doctored docket IDs because format looks plausible.\n  - Inability to follow links or attachments: incorrectly scoring a submission as low\u2011evidence because links are to paywalled PDFs or attachments the model cannot retrieve.\n  - Overrating quantity over quality: giving high scores to submissions listing many low\u2011credibility links (blogs, press releases) instead of a few strong sources.\n  - Misinterpreting statistical claims: failing to spot misuse of statistics (e.g., incorrect denominators, conflating percentage points with percent change) and overrating evidence quality.\n  - Failure to detect cherry\u2011picking: not recognizing when numbers are selectively presented without acknowledging counterevidence or uncertainty.\n  - Inconsistency across similar inputs: scoring similar submissions differently due to subtle wording changes that trigger different internal heuristics.\n  - Context blindness: treating a legal citation as authoritative even when jurisdiction or statutory context makes it irrelevant to the agency's authority.\n  - Adversarial formatting: being manipulated by maliciously formatted text (fake URLs, embedded citations in images) to inflate a submission's score.\n  - Over\u2011penalizing qualitative evidence: incorrectly assigning the lowest scores to persuasive firsthand accounts because they lack traditional citations.\n  - Undervaluing original analysis: penalizing submissions that include novel calculations because the judge cannot reproduce methods or validate datasets.\n\n## Related Metrics\n\n- **Related Metrics:**\n  - **LevenshteinDistance:** Levenshtein Distance measures the minimum number of single-character edits\u2014insertions, deletions, or substitutions\u2014required to transform one sequence into another.\n  - **BARTScore:** BARTScore is a reference-based evaluation metric for text generation that formulates evaluation as a text generation task.\n  - **PseudoPARENT:** **PseudoPARENT** is a *custom adaptation* of the PARENT metric for evaluating text generation from structured inputs.\n\n## Further Reading\n\n- **Papers:**\n  - [Autometrics](https://github.com/XenonMolecule/autometrics)\n  - [Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena](https://openreview.net/pdf?id=uccHPGDlao)\n\n## Citation\n\n```\n@software{Ryan_Autometrics_2025,\n    author = {Ryan, Michael J. and Zhang, Yanzhe and Salunkhe, Amol and Chu, Yi and Rahman, Emily and Xu, Di and Yang, Diyi},\n    license = {MIT},\n    title = {{Autometrics}},\n    url = {https://github.com/XenonMolecule/autometrics},\n    version = {1.0.0},\n    year = {2025}\n}\n```\n\n## Metric Card Authors\n\n- **Authors:** This metric card was automatically generated by gpt-5.\n- **Acknowledgement of AI Assistance:** This metric card was entirely automatically generated by gpt-5 using the Autometrics library. No human intervention was involved. User discretion is advised.\n- **Contact:** For questions about the autometrics library, please contact [Michael J Ryan](mailto:mryan0@stanford.edu).", "Clarity_Structure_and_Procedural_Completeness_Rubric": "---\n# Metric Card for Clarity_Structure_and_Procedural_Completeness_Rubric\n\n**Clarity Structure and Procedural Completeness** Organization, readability, civility, correct docket identifiers, accessible attachments, and contact details that facilitate agency action.\n\n## Metric Details\n\n**Clarity_Structure_and_Procedural_Completeness_Rubric** is a **reference-free** LLM-as-a-Judge metric that prompts an LLM to rate a system output along a single, run-time-specified evaluation axis.\nIn this case the axis is `**Clarity Structure and Procedural Completeness** Organization, readability, civility, correct docket identifiers, accessible attachments, and contact details that facilitate agency action.`.\n\nThe prompt supplies:\n\n1. **Task description** *d*\n2. **Rubric** `**Clarity Structure and Procedural Completeness** Organization, readability, civility, correct docket identifiers, accessible attachments, and contact details that facilitate agency action.`\n3. **Input text** *x*\n4. **Output text** *y*\n\nGreedy decoding (temperature = 0) yields an integer score $\\hat{s}\\!\\in\\!\\{1,2,3,4,5\\}$; higher = better adherence to the axis.\n\n- **Metric Type:** LLM as a Judge\n- **Range:** 1-5 (1 = worst, 5 = best)\n- **Higher is Better?:** Yes\n- **Reference-Based?:** No\n- **Input-Required?:** Yes\n\n### Formal Definition\n\nLet $f _{\\theta}$ be the LLM and\n$\\pi _{\text{RF}}(d,\\{axis\\},x,y)$ construct the textual prompt.\n\n$$\n\\hat{s} \\;=\\; \\operatorname*{arg\\,max}\\limits_{s \\in \\{1,\\dots,5\\}} f _{\theta}\\!\bigl(s \\,\bigl|\\, \\pi _{\text{RF}}(d,\\{axis\\},x,y)\bigr)\n$$\n\nThe metric value is $\\operatorname{LJ}^{\text{RF}}_{\\{axis\\}}(d,x,y)=\\hat{s}$.\n\n### Rubric Details\n\n**Criteria:** **Clarity Structure and Procedural Completeness** Organization, readability, civility, correct docket identifiers, accessible attachments, and contact details that facilitate agency action.\n\n#### Scoring Rubric\n\n| Score | Description |\n|-------|-------------|\n| 1 | Score 1 \u2014 Deficient<br/>\u2022 Disorganized or largely unintelligible; no logical flow.<br/>\u2022 Lacks civility or uses hostile/abusive language.<br/>\u2022 No docket ID or rule identifier; or clearly wrong action referenced.<br/>\u2022 Mentions attachments but none provided or they are inaccessible (e.g., image-only scans without text, broken links).<br/>\u2022 No contact information (name/affiliation/email/phone) for follow-up.<br/>\u2022 Formatting severely impedes readability (garbled text, excessive errors). |\n| 2 | Score 2 \u2014 Poor<br/>\u2022 Minimal structure; difficult to follow; arguments scattered.<br/>\u2022 Tone marginally civil or unnecessarily inflammatory.<br/>\u2022 Docket/rule reference is missing or ambiguous (e.g., incorrect number, only a vague topic).<br/>\u2022 Attachments referenced but incomplete, unlabeled, or not readily accessible/searchable.<br/>\u2022 Contact info is incomplete (e.g., name only, no way to reach submitter) or buried.<br/>\u2022 Formatting issues (missing headings, no pagination) hinder quick review. |\n| 3 | Score 3 \u2014 Adequate<br/>\u2022 Generally readable with a basic structure (intro/body/conclusion), but limited headings or summary.<br/>\u2022 Civil tone.<br/>\u2022 Includes either the correct docket ID or a clear rule/title reference; minor inconsistencies possible.<br/>\u2022 Attachments provided and mostly accessible, though with some formatting/searchability issues or sparse labeling.<br/>\u2022 Provides at least one reliable contact method (email or phone) and a name/affiliation.<br/>\u2022 Minor formatting gaps (e.g., no page numbers, inconsistent citation style) but content is usable. |\n| 4 | Score 4 \u2014 Strong<br/>\u2022 Well-organized: clear headings, logical flow, succinct paragraphs; possibly a brief summary of key points.<br/>\u2022 Consistently civil and professional.<br/>\u2022 Correct and specific docket ID and rule/title included in header or opening.<br/>\u2022 Attachments are clearly labeled, searchable (text-based), and referenced in-text; links work.<br/>\u2022 Complete contact block (name, affiliation, email, phone) and sign-off.<br/>\u2022 Clean formatting with pagination, dated letterhead or similar metadata; only minor, non-impeding omissions. |\n| 5 | Score 5 \u2014 Exemplary<br/>\u2022 Exemplary structure and readability: executive summary, numbered sections, clear asks/recommendations.<br/>\u2022 Unambiguously civil, respectful, and audience-appropriate.<br/>\u2022 Precise procedural details: correct docket ID(s), rule title, date, and any relevant agency office; consistent throughout.<br/>\u2022 Attachments and exhibits are fully accessible (searchable PDFs), clearly titled and cited in-text; hyperlinks and citations resolve; page and exhibit numbers provided.<br/>\u2022 Complete, prominent contact details (name, role/affiliation, email, phone, address) and a clear signature/date.<br/>\u2022 Accessibility and usability best practices: plain language, clear formatting, consistent citation style, alt text or descriptions where applicable, making it immediately actionable for agency officials. |\n\n### Inputs and Outputs\n- **Inputs:**\n  - **Task description** *d*\n  - **Rubric** `**Clarity Structure and Procedural Completeness** Organization, readability, civility, correct docket identifiers, accessible attachments, and contact details that facilitate agency action.`\n  - **Input text** *x*\n  - **Output text** *y*\n- **Outputs:**\n  - Scalar score $\\hat{s} \\in \\{1,2,3,4,5\\}$\n\n## Intended Use\n\n- **Domain:** Document Triage / Text Evaluation\n- **Tasks:** \n  - Triage public comments and citizen submissions for escalation to agency officials\n  - Score and label drafts for clarity, organization, and civility against the provided rubric\n  - Extract and validate presence/format of procedural identifiers (docket IDs, rule titles) from text\n  - Detect and flag missing or incomplete contact information and produce a follow-up checklist\n  - Assess attachments for accessibility when text-based content or searchable OCR is included\n  - Generate short summary or executive-notes highlighting deficiencies that block agency action\n  - Pre-screen submissions for potential procedural red flags (hostile tone, ambiguous docket references, inaccessible exhibits)\n- **Best Suited For:** \n  - Input documents are provided as machine-readable text or searchable OCR (not image-only scans).\n  - The rubric and expected docket ID formats are well-specified or follow consistent patterns the model can learn to recognize.\n  - The goal is rapid triage and prioritization rather than final legal judgment.\n  - Attachments are included inline or as clearly labeled text excerpts, enabling accessibility checks without external link fetching.\n  - Human reviewers are available to act on items the model flags as high-priority or ambiguous.\n- **Not Recommended For:** \n  - Attachments or evidence are only available via external links that the model cannot fetch or verify (broken links or remote files).\n  - Submissions are image-only scans without OCR or have low-quality scans that impede text extraction.\n  - Tasks require authoritative verification of docket numbers, filing status, or other facts that need access to agency databases or the internet.\n  - High-stakes legal interpretation, official determinations, identity/authenticity verification, or decisions with regulatory liability are required without human oversight.\n  - Multilingual inputs or domain-specific jargon without provided glossaries that might lead to tone and intent misclassification.\n\n## Metric Implementation\n\n### Reference Implementations\n\n- **Libraries/Packages:**\n  - [AutoMetrics LLM as a Judge (reference-free)](https://github.com/XenonMolecule/autometrics/blob/main/autometrics/metrics/generated/GeneratedLLMJudgeMetric.py)\n\n### Computational Complexity\n\n- **Efficiency:**\n  - Requires a single LLM call per input-output pair.\n  - AutoMetrics does parallel calls on batched inputs.\n\n- **Scalability:**\n  - Performance is linear in the number of input-output pairs.\n  - Performance depends on the underlying LLM model and the dataset size.  Additional consideration would include whether or not the LLM is a reasoning model.\n\n## Known Limitations\n\n- **Biases:** \n  - Formality bias: preferring professional, well-formatted language and downranking grassroots or informal but actionable submissions.\n  - Surface-cue bias: overweighting explicit metadata (docket numbers, headers) over implicit or paraphrased procedural references.\n  - Native-language bias: misjudging clarity or civility for non-native or dialectal phrasing as less readable or less civil.\n  - Civility detection errors: failing to detect sarcasm, coded hostility, or polite but hostile framing and thus mis-scoring tone.\n  - Attachment accessibility bias: penalizing attachments that are accessible to humans but not machine-readable (e.g., scanned PDFs without OCR).\n  - Conservatism/leniency calibration bias: systematic tendency to score more harshly or leniently depending on training examples or default thresholds.\n  - Template familiarity bias: rewarding submissions that follow common templates and penalizing novel but valid organizational formats.\n  - Confirmation/anchoring bias: initial cues (e.g., a clear docket ID) anchoring the score and causing the model to overlook other deficiencies.\n- **Task Misalignment Risks:** \n  - Over-prioritizing formatting over substance, causing highly substantive but informally presented comments to be under-escalated.\n  - Penalizing legitimate privacy-conscious submissions that intentionally omit full contact details, thereby missing actionable policy signals.\n  - Treating presence of a docket ID as sufficient for escalation even when the substantive content references the wrong proceeding or misunderstanding.\n  - Equating machine-readable attachments with real-world accessibility and ignoring context where a human reviewer could readily access materials.\n  - Reducing complex procedural nuance (e.g., comments intended for different agency office or multiple dockets) to a single score, losing important distinctions for escalation.\n  - Favoring submissions that match bureaucratic expectations and marginalizing nontraditional stakeholders (community groups, oral testimony transcripts).\n  - Applying rigid civility standards that could suppress escalation of urgent but emotionally charged citizen feedback.\n  - Failing to account for redacted or anonymized submissions that are nevertheless delegable to officials for policy significance.\n- **Failure Cases:** \n  - False negative: a clearly actionable submission is scored low because it uses informal structure and lacks a formal header.\n  - False positive: a well-formatted template letter with incorrect docket ID is scored highly and escalated despite pointing to the wrong rule.\n  - OCR/attachment failure: attachments are present as scanned images and are treatable as inaccessible, causing downgrading despite being readable to human reviewers.\n  - Link resolution failure: the model marks hyperlinks as broken because it cannot follow or validate them, even though they work in a browser.\n  - Civility misclassification: polite but critical language is flagged as hostile due to sarcasm or idiomatic expressions, lowering the civility score.\n  - Contact detection error: the model misses contact information that is present but embedded in a signature image or unconventional location.\n  - Docket parsing error: transposed digits or slight formatting differences cause the model to treat a correct docket reference as absent/incorrect.\n  - Inconsistent scoring: nearly identical submissions receive different scores across runs due to nondeterministic model outputs or subtle prompt sensitivity.\n  - Over-reliance on explicit headings: submissions with clear logical flow but without explicit headings are underrated relative to wordier submissions with headings.\n  - Accessibility overreach: the model downgrades submissions for not including alt text or accessibility metadata even when the agency does not require it, causing unnecessary non-escalation.\n  - Privacy-preserving submission missed: anonymous or redacted submissions with high policy relevance are rejected because contact details are missing.\n  - Hallucinated content: the model asserts the presence of attachments or contact details that do not exist, creating incorrect recommendations for escalation.\n\n## Related Metrics\n\n- **Related Metrics:**\n  - **LevenshteinDistance:** Levenshtein Distance measures the minimum number of single-character edits\u2014insertions, deletions, or substitutions\u2014required to transform one sequence into another.\n  - **BARTScore:** BARTScore is a reference-based evaluation metric for text generation that formulates evaluation as a text generation task.\n  - **PseudoPARENT:** **PseudoPARENT** is a *custom adaptation* of the PARENT metric for evaluating text generation from structured inputs.\n\n## Further Reading\n\n- **Papers:**\n  - [Autometrics](https://github.com/XenonMolecule/autometrics)\n  - [Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena](https://openreview.net/pdf?id=uccHPGDlao)\n\n## Citation\n\n```\n@software{Ryan_Autometrics_2025,\n    author = {Ryan, Michael J. and Zhang, Yanzhe and Salunkhe, Amol and Chu, Yi and Rahman, Emily and Xu, Di and Yang, Diyi},\n    license = {MIT},\n    title = {{Autometrics}},\n    url = {https://github.com/XenonMolecule/autometrics},\n    version = {1.0.0},\n    year = {2025}\n}\n```\n\n## Metric Card Authors\n\n- **Authors:** This metric card was automatically generated by gpt-5.\n- **Acknowledgement of AI Assistance:** This metric card was entirely automatically generated by gpt-5 using the Autometrics library. No human intervention was involved. User discretion is advised.\n- **Contact:** For questions about the autometrics library, please contact [Michael J Ryan](mailto:mryan0@stanford.edu).", "Specificity_of_Ask_and_Actionability_Rubric": "---\n# Metric Card for Specificity_of_Ask_and_Actionability_Rubric\n\n**Specificity of Ask and Actionability** Clarity of concrete requests or decisions sought from the agency, with steps the agency can realistically take.\n\n## Metric Details\n\n**Specificity_of_Ask_and_Actionability_Rubric** is a **reference-free** LLM-as-a-Judge metric that prompts an LLM to rate a system output along a single, run-time-specified evaluation axis.\nIn this case the axis is `**Specificity of Ask and Actionability** Clarity of concrete requests or decisions sought from the agency, with steps the agency can realistically take.`.\n\nThe prompt supplies:\n\n1. **Task description** *d*\n2. **Rubric** `**Specificity of Ask and Actionability** Clarity of concrete requests or decisions sought from the agency, with steps the agency can realistically take.`\n3. **Input text** *x*\n4. **Output text** *y*\n\nGreedy decoding (temperature = 0) yields an integer score $\\hat{s}\\!\\in\\!\\{1,2,3,4,5\\}$; higher = better adherence to the axis.\n\n- **Metric Type:** LLM as a Judge\n- **Range:** 1-5 (1 = worst, 5 = best)\n- **Higher is Better?:** Yes\n- **Reference-Based?:** No\n- **Input-Required?:** Yes\n\n### Formal Definition\n\nLet $f _{\\theta}$ be the LLM and\n$\\pi _{\text{RF}}(d,\\{axis\\},x,y)$ construct the textual prompt.\n\n$$\n\\hat{s} \\;=\\; \\operatorname*{arg\\,max}\\limits_{s \\in \\{1,\\dots,5\\}} f _{\theta}\\!\bigl(s \\,\bigl|\\, \\pi _{\text{RF}}(d,\\{axis\\},x,y)\bigr)\n$$\n\nThe metric value is $\\operatorname{LJ}^{\text{RF}}_{\\{axis\\}}(d,x,y)=\\hat{s}$.\n\n### Rubric Details\n\n**Criteria:** **Specificity of Ask and Actionability** Clarity of concrete requests or decisions sought from the agency, with steps the agency can realistically take.\n\n#### Scoring Rubric\n\n| Score | Description |\n|-------|-------------|\n| 1 | Score 1: No clear ask or not actionable<br/>\u2022 No explicit request or decision sought; purely opinion/rhetoric.<br/>\u2022 Requests are unrelated to the docket/topic or outside agency authority.<br/>\u2022 Lacks any concrete step, timeline, or responsible entity.<br/>\u2022 No references to rule text, docket, or statutory/regulatory levers.<br/>\u2022 Demands broad outcomes (e.g., \u201cfix pollution\u201d) without means. |\n| 2 | Score 2: Implicit or vague ask; low actionability<br/>\u2022 General support/oppose posture with at most a broad, non-specific request.<br/>\u2022 Asks framed in vague terms (\u201cconsider,\u201d \u201cdo better\u201d) without how-to.<br/>\u2022 Proposed actions are unrealistic or largely outside agency control.<br/>\u2022 No identification of who at the agency should act or when.<br/>\u2022 Minimal linkage to docket/sections; little to no evidence or rationale. |\n| 3 | Score 3: Clear primary ask but limited operational detail<br/>\u2022 A specific decision or direction is stated (e.g., retain/withdraw a proposal).<br/>\u2022 Feasible within agency authority, but implementation steps are thin.<br/>\u2022 May name the docket/rule, but lacks citations to sections or draft text.<br/>\u2022 Little specificity on timelines, metrics, or responsible office.<br/>\u2022 Few or no alternatives/contingencies; limited evidentiary support. |\n| 4 | Score 4: Concrete, feasible asks with clear implementation guidance<br/>\u2022 Multiple specific requests tied to the docket/rule sections or program area.<br/>\u2022 Identifies the responsible office/unit and suggests realistic steps (e.g., revise label language; exclude retrospective reviews; include co-benefits in BCA).<br/>\u2022 Cites relevant statutes/caselaw/precedents; provides supporting data.<br/>\u2022 May propose thresholds, criteria, or process steps; suggests timelines.<br/>\u2022 Some ambiguity remains (e.g., no draft text or detailed monitoring plan). |\n| 5 | Score 5: Highly specific and fully actionable plan<br/>\u2022 Enumerated asks each tied to exact levers (e.g., CFR/docket section edits, guidance updates, label text), with proposed wording or redlines.<br/>\u2022 Assigns responsibility (office/role), timeline/milestones, and success metrics/thresholds.<br/>\u2022 Grounds requests in statutory authority and constraints; includes feasible alternatives/fallbacks.<br/>\u2022 Anticipates implementation (e.g., data needs, reporting, stakeholder engagement, review panels) and proposes verification/monitoring.<br/>\u2022 Clearly within agency remit and immediately executable; provides contact and commitment to assist. |\n\n### Inputs and Outputs\n- **Inputs:**\n  - **Task description** *d*\n  - **Rubric** `**Specificity of Ask and Actionability** Clarity of concrete requests or decisions sought from the agency, with steps the agency can realistically take.`\n  - **Input text** *x*\n  - **Output text** *y*\n- **Outputs:**\n  - Scalar score $\\hat{s} \\in \\{1,2,3,4,5\\}$\n\n## Intended Use\n\n- **Domain:** Policy evaluation / Regulatory comment triage / Text classification\n- **Tasks:** \n  - Triage and prioritization of public comments for agency escalation\n  - Automated scoring of policy feedback drafts against a rubric (specificity/actionability)\n  - Highlighting and extracting explicit asks, timelines, and responsible offices from submissions\n  - Generating summary rationales for escalation decisions to assist human reviewers\n  - Batch processing and ranking of candidate responses for docket-focused review\n- **Best Suited For:** \n  - High-volume review contexts where consistent, rubric-based scoring is required to surface the most actionable submissions for human attention.\n  - Submissions in English with reasonably clear prose and standard regulatory vocabulary (docket IDs, section references, statutory citations).\n  - When the agency can provide the model with contextual metadata (docket number, relevant rule text, examples of scored comments) to calibrate judgments.\n  - Triage workflows that require highlighting exact language supporting a score (e.g., explicit wording of asks, named offices, timelines) rather than final legal determinations.\n  - Scenarios where the goal is to prioritize or summarize items for legal/policy staff who will conduct deeper review and decide on escalation.\n- **Not Recommended For:** \n  - Cases that require definitive legal interpretation of statutes, delegation-of-authority questions, or binding legal advice without human legal review.\n  - Highly technical scientific or engineering feasibility assessments that require domain specialists and empirical validation.\n  - Comments in languages other than those the model was trained/tuned on, or submissions with heavy use of nonstandard abbreviations or poor OCR quality.\n  - Situations with extremely low tolerance for hallucination (e.g., automatic redlining or issuing directives) where outputs would be acted on without human oversight.\n  - Environments lacking representative training examples or contextual docket/rule information, which increases risks of inconsistent or inaccurate scoring.\n\n## Metric Implementation\n\n### Reference Implementations\n\n- **Libraries/Packages:**\n  - [AutoMetrics LLM as a Judge (reference-free)](https://github.com/XenonMolecule/autometrics/blob/main/autometrics/metrics/generated/GeneratedLLMJudgeMetric.py)\n\n### Computational Complexity\n\n- **Efficiency:**\n  - Requires a single LLM call per input-output pair.\n  - AutoMetrics does parallel calls on batched inputs.\n\n- **Scalability:**\n  - Performance is linear in the number of input-output pairs.\n  - Performance depends on the underlying LLM model and the dataset size.  Additional consideration would include whether or not the LLM is a reasoning model.\n\n## Known Limitations\n\n- **Biases:** \n  - Surface-form bias: overvaluing explicit citations, docket numbers, or legalistic phrasing even when underlying requests are impractical.\n  - Recency and training-data bias: relying on patterns in training corpora that may not reflect current statutes, agency structures, or recent rulemaking practices.\n  - Formality bias: favoring submissions that use technical or formal language and penalizing plain-language citizen input regardless of substance.\n  - Jurisdictional bias: assuming common agency responsibilities or structures and misattributing which office/unit can act.\n  - Conservatism bias: penalizing novel or unconventional but feasible implementation ideas because they deviate from familiar templates.\n  - Confirmation bias from keywords: interpreting presence of certain words (e.g., 'CFR', 'docket', 'redline') as proxies for high actionability.\n  - Language and cultural bias: lower scoring for submissions that use different rhetorical styles, idioms, or non-native-English phrasing.\n  - Overconfidence bias: producing confident judgments about legal or operational feasibility despite lacking authoritative verification.\n- **Task Misalignment Risks:** \n  - Keyword overfitting: optimizing for presence of axis-specific words (docket, CFR, redline) rather than true operational specificity, misaligning evaluation with real-world implementability.\n  - Scope mismatch: treating any detailed request as actionable even when it requires funding, interagency coordination, or statutory change outside the agency's authority.\n  - Overemphasis on formal legal markers: penalizing valid citizen asks that propose practical, program-level changes without formal citations.\n  - Inflexible granularity: applying the same specificity standard to short public comments and to long policy memos, producing unfair comparisons.\n  - Neglect of feasibility constraints: scoring high on 'specificity' without assessing whether the agency has the personnel, budget, or statutory authority to execute the plan.\n  - Single-axis tunnel vision: ignoring other relevant evaluation axes (e.g., equity, evidence quality, legal risk), causing escalation of technically specific but harmful or legally invalid requests.\n  - Misinterpretation of delegated asks: failing to detect when the citizen directs action to another entity (state, private sector), and thus mis-scoring agency-relevance.\n  - Temporal mismatch: rewarding immediate-execution specificity when the agency's appropriate response is long-term rulemaking or research, not immediate operational changes.\n- **Failure Cases:** \n  - False positive: a comment includes detailed-sounding redlines and office names but proposes actions that violate statute or require Congressional appropriation, yet receives a top score.\n  - False negative: a succinct, plain-language submission that names a feasible procedural change (e.g., alter intake workflow) lacks citations and is scored low despite being immediately implementable.\n  - Inconsistent calibration: near-duplicate comments with minor wording differences get different scores because the model latches onto differing surface tokens.\n  - Gaming: submitters deliberately inject legalistic boilerplate (fake docket numbers, mock citations) to get higher escalation scores which the model cannot reliably verify.\n  - Jurisdiction error: the model assigns responsibility to the wrong office/unit (e.g., regional enforcement vs. rulemaking staff) leading to improper escalation routing.\n  - Context miss: the model fails to recognize that a requested metric or timeline is unrealistic given known rulemaking timelines and scores it as actionable.\n  - Ambiguity failure: when a comment contains multiple asks with mixed levels of specificity, the model either averages out and obscures the actionable element or incorrectly focuses on the least actionable part.\n  - Adversarial phrasing: politically charged or rhetorical language masks concrete asks that the model dismisses as opinion, missing otherwise actionable items.\n  - Evidence-check failure: the model accepts citations or data claims at face value and rates an ask higher despite the underlying evidence being irrelevant or fabricated.\n  - Cross-axis conflict: the model escalates a highly specific but legally risky proposal because it optimizes for this axis alone, ignoring legal risk that would disqualify escalation.\n\n## Related Metrics\n\n- **Related Metrics:**\n  - **LevenshteinDistance:** Levenshtein Distance measures the minimum number of single-character edits\u2014insertions, deletions, or substitutions\u2014required to transform one sequence into another.\n  - **BARTScore:** BARTScore is a reference-based evaluation metric for text generation that formulates evaluation as a text generation task.\n  - **PseudoPARENT:** **PseudoPARENT** is a *custom adaptation* of the PARENT metric for evaluating text generation from structured inputs.\n\n## Further Reading\n\n- **Papers:**\n  - [Autometrics](https://github.com/XenonMolecule/autometrics)\n  - [Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena](https://openreview.net/pdf?id=uccHPGDlao)\n\n## Citation\n\n```\n@software{Ryan_Autometrics_2025,\n    author = {Ryan, Michael J. and Zhang, Yanzhe and Salunkhe, Amol and Chu, Yi and Rahman, Emily and Xu, Di and Yang, Diyi},\n    license = {MIT},\n    title = {{Autometrics}},\n    url = {https://github.com/XenonMolecule/autometrics},\n    version = {1.0.0},\n    year = {2025}\n}\n```\n\n## Metric Card Authors\n\n- **Authors:** This metric card was automatically generated by gpt-5.\n- **Acknowledgement of AI Assistance:** This metric card was entirely automatically generated by gpt-5 using the Autometrics library. No human intervention was involved. User discretion is advised.\n- **Contact:** For questions about the autometrics library, please contact [Michael J Ryan](mailto:mryan0@stanford.edu).", "Source_Credibility_and_Representation_Rubric": "---\n# Metric Card for Source_Credibility_and_Representation_Rubric\n\n**Source Credibility and Representation** Author expertise, organizational standing, coalition support, and whether the comment represents large or affected constituencies.\n\n## Metric Details\n\n**Source_Credibility_and_Representation_Rubric** is a **reference-free** LLM-as-a-Judge metric that prompts an LLM to rate a system output along a single, run-time-specified evaluation axis.\nIn this case the axis is `**Source Credibility and Representation** Author expertise, organizational standing, coalition support, and whether the comment represents large or affected constituencies.`.\n\nThe prompt supplies:\n\n1. **Task description** *d*\n2. **Rubric** `**Source Credibility and Representation** Author expertise, organizational standing, coalition support, and whether the comment represents large or affected constituencies.`\n3. **Input text** *x*\n4. **Output text** *y*\n\nGreedy decoding (temperature = 0) yields an integer score $\\hat{s}\\!\\in\\!\\{1,2,3,4,5\\}$; higher = better adherence to the axis.\n\n- **Metric Type:** LLM as a Judge\n- **Range:** 1-5 (1 = worst, 5 = best)\n- **Higher is Better?:** Yes\n- **Reference-Based?:** No\n- **Input-Required?:** Yes\n\n### Formal Definition\n\nLet $f _{\\theta}$ be the LLM and\n$\\pi _{\text{RF}}(d,\\{axis\\},x,y)$ construct the textual prompt.\n\n$$\n\\hat{s} \\;=\\; \\operatorname*{arg\\,max}\\limits_{s \\in \\{1,\\dots,5\\}} f _{\theta}\\!\bigl(s \\,\bigl|\\, \\pi _{\text{RF}}(d,\\{axis\\},x,y)\bigr)\n$$\n\nThe metric value is $\\operatorname{LJ}^{\text{RF}}_{\\{axis\\}}(d,x,y)=\\hat{s}$.\n\n### Rubric Details\n\n**Criteria:** **Source Credibility and Representation** Author expertise, organizational standing, coalition support, and whether the comment represents large or affected constituencies.\n\n#### Scoring Rubric\n\n| Score | Description |\n|-------|-------------|\n| 1 | Score 1 (Very Low credibility/representation):<br/>\u2022 Anonymous or unclear author identity; no affiliation or role provided.<br/>\u2022 No credentials or evidence of domain expertise.<br/>\u2022 No claim of representing others; purely individual opinion with no indication of being affected.<br/>\u2022 No contact info, letterhead, or docket-specific context.<br/>\u2022 Signals of astroturfing/misrepresentation or generic, mass-produced text without source details. |\n| 2 | Score 2 (Low credibility/limited representation):<br/>\u2022 Identified individual with minimal context (e.g., resident, voter) and no relevant credentials.<br/>\u2022 May claim personal affectedness but provides no specifics or substantiation.<br/>\u2022 No organizational affiliation or coalition support.<br/>\u2022 Limited transparency (e.g., name but no role/contact), or weak alignment to the docket domain. |\n| 3 | Score 3 (Moderate credibility OR representation):<br/>\u2022 EITHER an individual with clear, relevant expertise (e.g., professional title, degree) but not representing a broader constituency,<br/>\u2022 OR a small/local organization (community group, local nonprofit) with some members/clients explicitly mentioned.<br/>\u2022 Some transparency (contact info/letterhead) and clear relevance to the docket.<br/>\u2022 No coalition support, or support from only one additional allied group.<br/>\u2022 Demonstrates some proximity to impacts (local effects, member experience) but limited scope. |\n| 4 | Score 4 (High credibility and/or substantial representation):<br/>\u2022 Recognized organization (regional/national association, NGO, trade group, agency, academic center) with stated membership or stakeholder base; or an author with strong, directly relevant credentials (e.g., PhD, executive role) writing on behalf of the organization.<br/>\u2022 Provides quantitative or specific representation claims (e.g., number of members, jurisdictions served) and shows direct affectedness (e.g., regulated entities, frontline communities).<br/>\u2022 Good transparency: letterhead, full contact info, docket citations, signatory roles.<br/>\u2022 May include multiple co-signers or endorsements, though coalition breadth is limited (e.g., a few allied orgs).<br/>\u2022 Clear mission and domain alignment to the issue. |\n| 5 | Score 5 (Very high credibility AND broad/affected representation with coalition strength):<br/>\u2022 Major national or multi-regional organization(s) or coalition letter with numerous and/or diverse signatories (industry, municipalities, NGOs, labor, community-based orgs) clearly listed.<br/>\u2022 Explicit, substantial constituency reach (e.g., thousands of members, majority of sector coverage, large populations served) or uniquely high affectedness (e.g., utilities under the rule, heavily burdened communities) documented in the submission.<br/>\u2022 Author signatories hold senior roles and/or possess strong domain expertise (e.g., executives, technical directors, PhDs), writing in their official capacities.<br/>\u2022 Full transparency (official letterhead, contacts, docket references) and strong domain relevance.<br/>\u2022 May reference prior formal engagements (testimony, advisory panels) or provide organization-generated data, indicating established policy participation. |\n\n### Inputs and Outputs\n- **Inputs:**\n  - **Task description** *d*\n  - **Rubric** `**Source Credibility and Representation** Author expertise, organizational standing, coalition support, and whether the comment represents large or affected constituencies.`\n  - **Input text** *x*\n  - **Output text** *y*\n- **Outputs:**\n  - Scalar score $\\hat{s} \\in \\{1,2,3,4,5\\}$\n\n## Intended Use\n\n- **Domain:** Policy feedback evaluation / Text classification\n- **Tasks:** \n  - Triage of public comments for escalation to agency officials\n  - Credibility scoring of policy feedback submissions\n  - Identifying and enumerating signatories and coalition breadth in letters\n  - Flagging potential astroturfing or mass-produced comments\n  - Prioritizing high-impact stakeholder inputs for reviewer attention\n  - Generating short rationales tied to rubric criteria for each score\n  - Filtering submissions that require human verification or follow-up\n- **Best Suited For:** \n  - Submissions primarily in English and in standard textual formats (letters, comments, emails) where identity, affiliation, and signatory information are present or omitted explicitly.\n  - Large volumes of comments requiring consistent, repeatable triage according to a transparent rubric.\n  - Situations where the goal is to prioritize which submissions merit human escalation rather than to conclusively verify external claims.\n  - When comments include structured cues (letterhead, contact blocks, explicit membership counts, docket citations, lists of co-signers) that are directly mappable to the scoring guidelines.\n  - Contexts that benefit from quick, explainable justifications for scores (e.g., to create human-review queues or summary dashboards).\n- **Not Recommended For:** \n  - Cases that require external verification of claims (membership numbers, official roles, prior testimony) or authoritative fact-checking \u2014 the model cannot reliably confirm off-document facts.\n  - Submissions that are multimedia-heavy (scanned images of letters, audio/video testimony) without accurate OCR/transcription, or texts in languages the model is weak in.\n  - High-stakes legal or policy decisions where incorrect credibility assessments could cause material harm and thus require domain-expert adjudication.\n  - Highly adversarial or purposely deceptive inputs designed to spoof organizational credentials or fabricate coalitions without corroborating evidence.\n  - Narrow, highly technical domains with uncommon institutional codewords or localized organizational structures the model may not recognize without additional context.\n\n## Metric Implementation\n\n### Reference Implementations\n\n- **Libraries/Packages:**\n  - [AutoMetrics LLM as a Judge (reference-free)](https://github.com/XenonMolecule/autometrics/blob/main/autometrics/metrics/generated/GeneratedLLMJudgeMetric.py)\n\n### Computational Complexity\n\n- **Efficiency:**\n  - Requires a single LLM call per input-output pair.\n  - AutoMetrics does parallel calls on batched inputs.\n\n- **Scalability:**\n  - Performance is linear in the number of input-output pairs.\n  - Performance depends on the underlying LLM model and the dataset size.  Additional consideration would include whether or not the LLM is a reasoning model.\n\n## Known Limitations\n\n- **Biases:** \n  - Credentialism bias: privileging formal academic/professional credentials over lived experience or community knowledge.\n  - Institutional/status bias: favoring large, well-known organizations and letterhead, disadvantaging small grassroots groups.\n  - Format/visual bias: treating presence of letterhead, full contact info, or formal formatting as proxies for credibility.\n  - Language and cultural bias: penalizing submissions from non-native English speakers or different rhetorical styles.\n  - Name-recognition bias: giving extra weight to familiar institution names or senior-sounding titles.\n  - Coalition-size bias: equating larger signatory lists with substantive representation without verifying diversity or authenticity.\n  - Anonymity-averse bias: automatically lowering scores for anonymous submissions even when anonymity protects vulnerable individuals.\n  - Surface-evidence bias: overrelying on explicit numeric claims (member counts, jurisdictions) that may be unverified or inflated.\n  - Confirmation bias: interpreting ambiguous claims in a way that aligns with prior model exposure to similar documents.\n  - Recency/training-data bias: preferring organizational types and presentation styles common in the model's training set.\n- **Task Misalignment Risks:** \n  - Undervaluing affected individuals: the axis can systematically deprioritize individual but highly affected stakeholders who lack formal affiliation or credentials.\n  - Overvaluing self-interested groups: well-resourced industry or advocacy organizations may be scored high despite narrow or biased interests.\n  - Penalizing protected/vulnerable voices: anonymity or withheld contact info may reflect safety needs, not low credibility, leading to harmful exclusion.\n  - Misreading coalition breadth: counting signatories as representation without assessing whether signers reflect diverse or relevant constituencies.\n  - Conflating form with substance: the rubric may prioritize presentation quality (letterhead, citations) over factual accuracy or on-the-ground relevance.\n  - Domain mismatch: a single rubric may fail to account for domain-specific credibility signals (e.g., patents in tech vs lived experience in community health).\n  - False sense of verification: assigning high credibility based on claims the LLM cannot externally verify (membership numbers, prior testimony).\n  - Disincentivizing grassroots testimony: public procedures may favor institutional commenters, skewing agency attention away from frontline impacts.\n  - Ambiguity in multi-author submissions: the rubric may not specify how to score mixed-author inputs (expert co-signers plus anonymous community members).\n  - Overemphasis on coalition breadth could ignore depth of impact (a small but uniquely affected group may be more relevant than a large but peripheral coalition).\n- **Failure Cases:** \n  - Classifying an anonymous whistleblower's detailed, verifiable technical account as 'Very Low' because contact info is absent, thereby failing to escalate a critical issue.\n  - Elevating a polished coalition letter with fabricated signatories and inflated membership counts to 'Very High' because of formatting and large numbers.\n  - Downgrading grassroots submissions written in non-standard English or with culturally distinct rhetorical styles, missing important affectedness evidence.\n  - Failing to detect astroturfing when a lobby group submits many coordinated comments with slight variations, instead scoring them as broad grassroots representation.\n  - Scoring a small local nonprofit low because it lacks letterhead or formal membership numbers, despite the organization being the primary affected party.\n  - Treating long, data-heavy industry comments with unverifiable proprietary claims as high credibility while ignoring conflicts of interest.\n  - Missing domain nuance: giving a high score to an author with a PhD in an adjacent field whose expertise does not apply to the docket's technical area.\n  - Misinterpreting co-signer lists: counting numerous minor endorsements as evidence of broad representation when signers are duplicate or irrelevant entities.\n  - Penalizing a victim/survivor who omits contact info for privacy, thereby silencing a directly affected constituency in the escalation pipeline.\n  - Failing to flag obviously fake organizations with realistic-sounding names because the model overrelies on lexical heuristics rather than verification.\n\n## Related Metrics\n\n- **Related Metrics:**\n  - **LevenshteinDistance:** Levenshtein Distance measures the minimum number of single-character edits\u2014insertions, deletions, or substitutions\u2014required to transform one sequence into another.\n  - **BARTScore:** BARTScore is a reference-based evaluation metric for text generation that formulates evaluation as a text generation task.\n  - **PseudoPARENT:** **PseudoPARENT** is a *custom adaptation* of the PARENT metric for evaluating text generation from structured inputs.\n\n## Further Reading\n\n- **Papers:**\n  - [Autometrics](https://github.com/XenonMolecule/autometrics)\n  - [Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena](https://openreview.net/pdf?id=uccHPGDlao)\n\n## Citation\n\n```\n@software{Ryan_Autometrics_2025,\n    author = {Ryan, Michael J. and Zhang, Yanzhe and Salunkhe, Amol and Chu, Yi and Rahman, Emily and Xu, Di and Yang, Diyi},\n    license = {MIT},\n    title = {{Autometrics}},\n    url = {https://github.com/XenonMolecule/autometrics},\n    version = {1.0.0},\n    year = {2025}\n}\n```\n\n## Metric Card Authors\n\n- **Authors:** This metric card was automatically generated by gpt-5.\n- **Acknowledgement of AI Assistance:** This metric card was entirely automatically generated by gpt-5 using the Autometrics library. No human intervention was involved. User discretion is advised.\n- **Contact:** For questions about the autometrics library, please contact [Michael J Ryan](mailto:mryan0@stanford.edu)."}; const RC_DOCS_MAP = {}; const RC_PY_CODE = "# Auto-generated static regression for Autometrics_Regression_c\nfrom typing import ClassVar\nimport numpy as np\nfrom autometrics.aggregator.generated.GeneratedRegressionMetric import GeneratedStaticRegressionAggregator\n\n\n\n# Auto-generated metric file for Evidentiary_Support_and_Citations_Rubric\nimport dspy\nimport os\nfrom autometrics.metrics.generated.GeneratedLLMJudgeMetric import GeneratedRefFreeLLMJudgeMetric\nfrom typing import ClassVar\n\nDEFAULT_MODEL_Evidentiary_Support_and_Citations_Rubric_LLMJudge = dspy.LM(model='openai/gpt-5-mini', api_key=os.getenv(\"OPENAI_API_KEY\"))\n\nclass Evidentiary_Support_and_Citations_Rubric_LLMJudge(GeneratedRefFreeLLMJudgeMetric):\n    \"\"\"\"\"\"\n\n    description: ClassVar[str] = \"**Evidentiary Support and Citations** Use of data, analyses, studies, and references that substantiate claims and quantify effects.\"\n\n    def __init__(self, model: dspy.LM = DEFAULT_MODEL_Evidentiary_Support_and_Citations_Rubric_LLMJudge):\n        super().__init__(\n            name=\"Evidentiary_Support_and_Citations_Rubric\",\n            description=\"**Evidentiary Support and Citations** Use of data, analyses, studies, and references that substantiate claims and quantify effects.\",\n            axis=\"**Evidentiary Support and Citations** Use of data, analyses, studies, and references that substantiate claims and quantify effects.\\n\\nScoring Guidelines:\\nScore 1: Score 1: No evidentiary support\\n- Purely opinion/anecdote; no citations or links.\\n- No quantitative information supporting claims.\\n- References are irrelevant, unverifiable, or obviously inaccurate.\\n- \\u201cSee attached\\u201d or name-dropping without incorporating any evidence into the argument.\\n- Fabricated-sounding or conflicting facts with no sources.\\nScore 2: Score 2: Minimal, vague, or weak support\\n- One or two vague references (e.g., \\u201cstudies show,\\u201d \\u201cEPA reports\\u201d) without titles, dates, docket IDs, or URLs.\\n- Mentions numbers but provides no source or traceability.\\n- Sources are low-credibility (blogs, opinion pieces) or links are broken.\\n- Evidence is not integrated into reasoning; limited or incorrect interpretation of cited material.\\nScore 3: Score 3: Adequate baseline support\\n- At least one specific, credible citation (e.g., docket ID, Federal Register cite, named report/study with year).\\n- Includes some quantitative data tied to a source.\\n- Evidence generally supports key claims, though coverage may be partial or selective.\\n- Minor gaps in traceability (missing page numbers, incomplete links) or reliance on secondary summaries.\\n- Limited discussion of methods/limitations; little triangulation.\\nScore 4: Score 4: Strong, well-integrated support\\n- Multiple credible, traceable sources (e.g., peer-reviewed studies, government reports, statutes/caselaw, docket materials) with precise identifiers (titles, dates, page/section numbers, URLs).\\n- Quantifies effects clearly (units, magnitudes, time frames) and ties numbers to sources.\\n- Evidence is accurately interpreted and woven into arguments that inform agency action.\\n- Notes key assumptions/limitations; some triangulation across independent sources.\\n- Attachments/exhibits referenced in-text; links work; minimal inconsistencies.\\nScore 5: Score 5: Exceptional, decision-grade evidence use\\n- Comprehensive, diverse evidence base (primary studies, official data, legal/technical references) with precise, reproducible citations (page/figure numbers, dataset names, docket IDs).\\n- Provides original analysis or calculations (methods shown, equations/assumptions stated), data tables/appendices, or replicable methodology.\\n- Quantifies impacts rigorously (comparative baselines, confidence/uncertainty where applicable) and addresses counter-evidence.\\n- Clearly links evidence to concrete recommendations/options, showing decision relevance.\\n- All references are authoritative and accessible; no traceability gaps; consistent and accurate interpretation throughout.\\n\",\n            model=model,\n            task_description=\"Rank candidate policy feedback drafts for follow-up: given a citizen\\u2019s submission, \\n    determine which responses merit escalation to agency officials.\",\n            metric_card=\"None\",\n            max_workers=32,\n        )\n\n    def __repr__(self):\n        return f\"Evidentiary_Support_and_Citations_Rubric_LLMJudge(model=dspy.LM(model='openai/gpt-5-mini', api_key=os.getenv(\\\"OPENAI_API_KEY\\\")))\"\n\n\n\n# Auto-generated metric file for Clarity_Structure_and_Procedural_Completeness_Rubric\nimport dspy\nimport os\nfrom autometrics.metrics.generated.GeneratedLLMJudgeMetric import GeneratedRefFreeLLMJudgeMetric\nfrom typing import ClassVar\n\nDEFAULT_MODEL_Clarity_Structure_and_Procedural_Completeness_Rubric_LLMJudge = dspy.LM(model='openai/gpt-5-mini', api_key=os.getenv(\"OPENAI_API_KEY\"))\n\nclass Clarity_Structure_and_Procedural_Completeness_Rubric_LLMJudge(GeneratedRefFreeLLMJudgeMetric):\n    \"\"\"\"\"\"\n\n    description: ClassVar[str] = \"**Clarity Structure and Procedural Completeness** Organization, readability, civility, correct docket identifiers, accessible attachments, and contact details that facilitate agency action.\"\n\n    def __init__(self, model: dspy.LM = DEFAULT_MODEL_Clarity_Structure_and_Procedural_Completeness_Rubric_LLMJudge):\n        super().__init__(\n            name=\"Clarity_Structure_and_Procedural_Completeness_Rubric\",\n            description=\"**Clarity Structure and Procedural Completeness** Organization, readability, civility, correct docket identifiers, accessible attachments, and contact details that facilitate agency action.\",\n            axis=\"**Clarity Structure and Procedural Completeness** Organization, readability, civility, correct docket identifiers, accessible attachments, and contact details that facilitate agency action.\\n\\nScoring Guidelines:\\nScore 1: Score 1 \\u2014 Deficient\\n- Disorganized or largely unintelligible; no logical flow.\\n- Lacks civility or uses hostile/abusive language.\\n- No docket ID or rule identifier; or clearly wrong action referenced.\\n- Mentions attachments but none provided or they are inaccessible (e.g., image-only scans without text, broken links).\\n- No contact information (name/affiliation/email/phone) for follow-up.\\n- Formatting severely impedes readability (garbled text, excessive errors).\\nScore 2: Score 2 \\u2014 Poor\\n- Minimal structure; difficult to follow; arguments scattered.\\n- Tone marginally civil or unnecessarily inflammatory.\\n- Docket/rule reference is missing or ambiguous (e.g., incorrect number, only a vague topic).\\n- Attachments referenced but incomplete, unlabeled, or not readily accessible/searchable.\\n- Contact info is incomplete (e.g., name only, no way to reach submitter) or buried.\\n- Formatting issues (missing headings, no pagination) hinder quick review.\\nScore 3: Score 3 \\u2014 Adequate\\n- Generally readable with a basic structure (intro/body/conclusion), but limited headings or summary.\\n- Civil tone.\\n- Includes either the correct docket ID or a clear rule/title reference; minor inconsistencies possible.\\n- Attachments provided and mostly accessible, though with some formatting/searchability issues or sparse labeling.\\n- Provides at least one reliable contact method (email or phone) and a name/affiliation.\\n- Minor formatting gaps (e.g., no page numbers, inconsistent citation style) but content is usable.\\nScore 4: Score 4 \\u2014 Strong\\n- Well-organized: clear headings, logical flow, succinct paragraphs; possibly a brief summary of key points.\\n- Consistently civil and professional.\\n- Correct and specific docket ID and rule/title included in header or opening.\\n- Attachments are clearly labeled, searchable (text-based), and referenced in-text; links work.\\n- Complete contact block (name, affiliation, email, phone) and sign-off.\\n- Clean formatting with pagination, dated letterhead or similar metadata; only minor, non-impeding omissions.\\nScore 5: Score 5 \\u2014 Exemplary\\n- Exemplary structure and readability: executive summary, numbered sections, clear asks/recommendations.\\n- Unambiguously civil, respectful, and audience-appropriate.\\n- Precise procedural details: correct docket ID(s), rule title, date, and any relevant agency office; consistent throughout.\\n- Attachments and exhibits are fully accessible (searchable PDFs), clearly titled and cited in-text; hyperlinks and citations resolve; page and exhibit numbers provided.\\n- Complete, prominent contact details (name, role/affiliation, email, phone, address) and a clear signature/date.\\n- Accessibility and usability best practices: plain language, clear formatting, consistent citation style, alt text or descriptions where applicable, making it immediately actionable for agency officials.\\n\",\n            model=model,\n            task_description=\"Rank candidate policy feedback drafts for follow-up: given a citizen\\u2019s submission, \\n    determine which responses merit escalation to agency officials.\",\n            metric_card=\"None\",\n            max_workers=32,\n        )\n\n    def __repr__(self):\n        return f\"Clarity_Structure_and_Procedural_Completeness_Rubric_LLMJudge(model=dspy.LM(model='openai/gpt-5-mini', api_key=os.getenv(\\\"OPENAI_API_KEY\\\")))\"\n\n\n\n# Auto-generated metric file for Specificity_of_Ask_and_Actionability_Rubric\nimport dspy\nimport os\nfrom autometrics.metrics.generated.GeneratedLLMJudgeMetric import GeneratedRefFreeLLMJudgeMetric\nfrom typing import ClassVar\n\nDEFAULT_MODEL_Specificity_of_Ask_and_Actionability_Rubric_LLMJudge = dspy.LM(model='openai/gpt-5-mini', api_key=os.getenv(\"OPENAI_API_KEY\"))\n\nclass Specificity_of_Ask_and_Actionability_Rubric_LLMJudge(GeneratedRefFreeLLMJudgeMetric):\n    \"\"\"\"\"\"\n\n    description: ClassVar[str] = \"**Specificity of Ask and Actionability** Clarity of concrete requests or decisions sought from the agency, with steps the agency can realistically take.\"\n\n    def __init__(self, model: dspy.LM = DEFAULT_MODEL_Specificity_of_Ask_and_Actionability_Rubric_LLMJudge):\n        super().__init__(\n            name=\"Specificity_of_Ask_and_Actionability_Rubric\",\n            description=\"**Specificity of Ask and Actionability** Clarity of concrete requests or decisions sought from the agency, with steps the agency can realistically take.\",\n            axis=\"**Specificity of Ask and Actionability** Clarity of concrete requests or decisions sought from the agency, with steps the agency can realistically take.\\n\\nScoring Guidelines:\\nScore 1: Score 1: No clear ask or not actionable\\n- No explicit request or decision sought; purely opinion/rhetoric.\\n- Requests are unrelated to the docket/topic or outside agency authority.\\n- Lacks any concrete step, timeline, or responsible entity.\\n- No references to rule text, docket, or statutory/regulatory levers.\\n- Demands broad outcomes (e.g., \\u201cfix pollution\\u201d) without means.\\nScore 2: Score 2: Implicit or vague ask; low actionability\\n- General support/oppose posture with at most a broad, non-specific request.\\n- Asks framed in vague terms (\\u201cconsider,\\u201d \\u201cdo better\\u201d) without how-to.\\n- Proposed actions are unrealistic or largely outside agency control.\\n- No identification of who at the agency should act or when.\\n- Minimal linkage to docket/sections; little to no evidence or rationale.\\nScore 3: Score 3: Clear primary ask but limited operational detail\\n- A specific decision or direction is stated (e.g., retain/withdraw a proposal).\\n- Feasible within agency authority, but implementation steps are thin.\\n- May name the docket/rule, but lacks citations to sections or draft text.\\n- Little specificity on timelines, metrics, or responsible office.\\n- Few or no alternatives/contingencies; limited evidentiary support.\\nScore 4: Score 4: Concrete, feasible asks with clear implementation guidance\\n- Multiple specific requests tied to the docket/rule sections or program area.\\n- Identifies the responsible office/unit and suggests realistic steps (e.g., revise label language; exclude retrospective reviews; include co-benefits in BCA).\\n- Cites relevant statutes/caselaw/precedents; provides supporting data.\\n- May propose thresholds, criteria, or process steps; suggests timelines.\\n- Some ambiguity remains (e.g., no draft text or detailed monitoring plan).\\nScore 5: Score 5: Highly specific and fully actionable plan\\n- Enumerated asks each tied to exact levers (e.g., CFR/docket section edits, guidance updates, label text), with proposed wording or redlines.\\n- Assigns responsibility (office/role), timeline/milestones, and success metrics/thresholds.\\n- Grounds requests in statutory authority and constraints; includes feasible alternatives/fallbacks.\\n- Anticipates implementation (e.g., data needs, reporting, stakeholder engagement, review panels) and proposes verification/monitoring.\\n- Clearly within agency remit and immediately executable; provides contact and commitment to assist.\\n\",\n            model=model,\n            task_description=\"Rank candidate policy feedback drafts for follow-up: given a citizen\\u2019s submission, \\n    determine which responses merit escalation to agency officials.\",\n            metric_card=\"None\",\n            max_workers=32,\n        )\n\n    def __repr__(self):\n        return f\"Specificity_of_Ask_and_Actionability_Rubric_LLMJudge(model=dspy.LM(model='openai/gpt-5-mini', api_key=os.getenv(\\\"OPENAI_API_KEY\\\")))\"\n\n\n\n# Auto-generated metric file for Source_Credibility_and_Representation_Rubric\nimport dspy\nimport os\nfrom autometrics.metrics.generated.GeneratedLLMJudgeMetric import GeneratedRefFreeLLMJudgeMetric\nfrom typing import ClassVar\n\nDEFAULT_MODEL_Source_Credibility_and_Representation_Rubric_LLMJudge = dspy.LM(model='openai/gpt-5-mini', api_key=os.getenv(\"OPENAI_API_KEY\"))\n\nclass Source_Credibility_and_Representation_Rubric_LLMJudge(GeneratedRefFreeLLMJudgeMetric):\n    \"\"\"\"\"\"\n\n    description: ClassVar[str] = \"**Source Credibility and Representation** Author expertise, organizational standing, coalition support, and whether the comment represents large or affected constituencies.\"\n\n    def __init__(self, model: dspy.LM = DEFAULT_MODEL_Source_Credibility_and_Representation_Rubric_LLMJudge):\n        super().__init__(\n            name=\"Source_Credibility_and_Representation_Rubric\",\n            description=\"**Source Credibility and Representation** Author expertise, organizational standing, coalition support, and whether the comment represents large or affected constituencies.\",\n            axis=\"**Source Credibility and Representation** Author expertise, organizational standing, coalition support, and whether the comment represents large or affected constituencies.\\n\\nScoring Guidelines:\\nScore 1: Score 1 (Very Low credibility/representation):\\n- Anonymous or unclear author identity; no affiliation or role provided.\\n- No credentials or evidence of domain expertise.\\n- No claim of representing others; purely individual opinion with no indication of being affected.\\n- No contact info, letterhead, or docket-specific context.\\n- Signals of astroturfing/misrepresentation or generic, mass-produced text without source details.\\nScore 2: Score 2 (Low credibility/limited representation):\\n- Identified individual with minimal context (e.g., resident, voter) and no relevant credentials.\\n- May claim personal affectedness but provides no specifics or substantiation.\\n- No organizational affiliation or coalition support.\\n- Limited transparency (e.g., name but no role/contact), or weak alignment to the docket domain.\\nScore 3: Score 3 (Moderate credibility OR representation):\\n- EITHER an individual with clear, relevant expertise (e.g., professional title, degree) but not representing a broader constituency,\\n- OR a small/local organization (community group, local nonprofit) with some members/clients explicitly mentioned.\\n- Some transparency (contact info/letterhead) and clear relevance to the docket.\\n- No coalition support, or support from only one additional allied group.\\n- Demonstrates some proximity to impacts (local effects, member experience) but limited scope.\\nScore 4: Score 4 (High credibility and/or substantial representation):\\n- Recognized organization (regional/national association, NGO, trade group, agency, academic center) with stated membership or stakeholder base; or an author with strong, directly relevant credentials (e.g., PhD, executive role) writing on behalf of the organization.\\n- Provides quantitative or specific representation claims (e.g., number of members, jurisdictions served) and shows direct affectedness (e.g., regulated entities, frontline communities).\\n- Good transparency: letterhead, full contact info, docket citations, signatory roles.\\n- May include multiple co-signers or endorsements, though coalition breadth is limited (e.g., a few allied orgs).\\n- Clear mission and domain alignment to the issue.\\nScore 5: Score 5 (Very high credibility AND broad/affected representation with coalition strength):\\n- Major national or multi-regional organization(s) or coalition letter with numerous and/or diverse signatories (industry, municipalities, NGOs, labor, community-based orgs) clearly listed.\\n- Explicit, substantial constituency reach (e.g., thousands of members, majority of sector coverage, large populations served) or uniquely high affectedness (e.g., utilities under the rule, heavily burdened communities) documented in the submission.\\n- Author signatories hold senior roles and/or possess strong domain expertise (e.g., executives, technical directors, PhDs), writing in their official capacities.\\n- Full transparency (official letterhead, contacts, docket references) and strong domain relevance.\\n- May reference prior formal engagements (testimony, advisory panels) or provide organization-generated data, indicating established policy participation.\\n\",\n            model=model,\n            task_description=\"Rank candidate policy feedback drafts for follow-up: given a citizen\\u2019s submission, \\n    determine which responses merit escalation to agency officials.\",\n            metric_card=\"None\",\n            max_workers=32,\n        )\n\n    def __repr__(self):\n        return f\"Source_Credibility_and_Representation_Rubric_LLMJudge(model=dspy.LM(model='openai/gpt-5-mini', api_key=os.getenv(\\\"OPENAI_API_KEY\\\")))\"\n\n\n\n\n\nINPUT_METRICS = [\n        Evidentiary_Support_and_Citations_Rubric_LLMJudge(),\n        Clarity_Structure_and_Procedural_Completeness_Rubric_LLMJudge(),\n        Specificity_of_Ask_and_Actionability_Rubric_LLMJudge(),\n        Source_Credibility_and_Representation_Rubric_LLMJudge()\n]\n\nclass Autometrics_Regression_c_StaticRegression(GeneratedStaticRegressionAggregator):\n    \"\"\"Regression aggregator over component metrics with a linear model.\n\nComponents and weights:\n- Evidentiary_Support_and_Citations_Rubric: 0.099141\n- Clarity_Structure_and_Procedural_Completeness_Rubric: 0.072882\n- Specificity_of_Ask_and_Actionability_Rubric: 0.005151\n- Source_Credibility_and_Representation_Rubric: -0.000000\n\nIntercept: 0.505000\"\"\"\n\n    description: ClassVar[str] = 'Regression aggregator over component metrics with a linear model.\\n\\nComponents and weights:\\n- Evidentiary_Support_and_Citations_Rubric: 0.099141\\n- Clarity_Structure_and_Procedural_Completeness_Rubric: 0.072882\\n- Specificity_of_Ask_and_Actionability_Rubric: 0.005151\\n- Source_Credibility_and_Representation_Rubric: -0.000000\\n\\nIntercept: 0.505000'\n\n    def __init__(self):\n        super().__init__(\n            name='Autometrics_Regression_c',\n            description='Regression aggregator over component metrics with a linear model.\\n\\nComponents and weights:\\n- Evidentiary_Support_and_Citations_Rubric: 0.099141\\n- Clarity_Structure_and_Procedural_Completeness_Rubric: 0.072882\\n- Specificity_of_Ask_and_Actionability_Rubric: 0.005151\\n- Source_Credibility_and_Representation_Rubric: -0.000000\\n\\nIntercept: 0.505000',\n            input_metrics=INPUT_METRICS,\n            feature_names=['Evidentiary_Support_and_Citations_Rubric', 'Clarity_Structure_and_Procedural_Completeness_Rubric', 'Specificity_of_Ask_and_Actionability_Rubric', 'Source_Credibility_and_Representation_Rubric'],\n            coefficients=[0.09914143738961213, 0.07288186958726793, 0.005151409890197831, -0.0],\n            intercept=0.505,\n            scaler_mean=[2.945, 3.325, 3.245, 3.96],\n            scaler_scale=[0.8496911203490359, 0.8302860952707807, 0.6441855322808795, 0.882269800004511],\n        )\n\n    def __repr__(self):\n        return f\"ElasticNet(name={repr(self.name)})\"\n"; const RC_PY_FILENAME = "Autometrics_Regression_c.py";</script>
</head>
<body>
  <div class="container my-5">
    <div class="d-flex justify-content-between align-items-center mb-4">
      <h1>C AutoMetric Report Card</h1>
      <div class="d-flex align-items-center">
        <div class="form-check form-switch me-3">
          <input class="form-check-input" type="checkbox" id="darkModeToggle">
          <label class="form-check-label" for="darkModeToggle">Dark Mode</label>
        </div>
        <button class="btn btn-primary" onclick="window.print()">Export to PDF</button>
        <button class="btn btn-outline-primary ms-2" type="button" onclick="downloadPython()">Export to Python</button>
      </div>
    </div>

    <div class="row g-4">
      <div class="col-md-6">
        <div class="card p-3 h-100">
          <h2>Regression Coefficients</h2>
          <table class="table table-striped"><thead><tr><th>Metric</th><th>Coeff.</th></tr></thead>
            <tbody><tr><td><a href="#" class="coeff-link" data-metric="Evidentiary_Support_and_Citations_Rubric">Evidentiary_Support_and_Citations_Rubric</a></td><td>0.0991</td></tr><tr><td><a href="#" class="coeff-link" data-metric="Clarity_Structure_and_Procedural_Completeness_Rubric">Clarity_Structure_and_Procedural_Completeness_Rubric</a></td><td>0.0729</td></tr><tr><td><a href="#" class="coeff-link" data-metric="Specificity_of_Ask_and_Actionability_Rubric">Specificity_of_Ask_and_Actionability_Rubric</a></td><td>0.0052</td></tr><tr><td><a href="#" class="coeff-link" data-metric="Source_Credibility_and_Representation_Rubric">Source_Credibility_and_Representation_Rubric</a></td><td>-0.0000</td></tr></tbody>
          </table>
        </div>
      </div>

      <div class="col-md-6">
        <div class="card p-3 h-100">
          <h2>Correlation</h2>
          <div id="correlation-chart" style="height:420px;"></div>
          <div id="correlation-stats" class="mt-2" style="text-align:center; font-size: 1rem; font-weight: 600;"></div>
        </div>
      </div>

      <div class="col-md-6">
        <div class="card p-3 h-100">
          <h2>Robustness <sup><span class="robust-tip text-primary" data-tip-id="robustness-tip-template" style="cursor:pointer; text-decoration: underline; font-size: 0.9rem;">?</span></sup></h2>
          <div id="robustness-sens" style="height:240px;"></div>
          <div id="robustness-stab" style="height:240px;"></div>
        </div>
      </div>

      <div class="col-md-6">
        <div class="card p-3 h-100">
          <h2>Run Time Distribution</h2>
          <div id="runtime-chart" style="height:300px;"></div>
          <p id="runtime-info" class="mt-2"></p>
        </div>
      </div>

      <div class="col-md-6">
        <div class="card p-3 h-100">
          <h2>Metric Details</h2>
          <div class="accordion" id="metricDetails">
            <div class="accordion-item">
              <h2 class="accordion-header" id="descHeader"><button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#descPanel">Descriptions</button></h2>
              <div id="descPanel" class="accordion-collapse collapse"><div class="accordion-body"><ul><li><strong>Evidentiary_Support_and_Citations_Rubric:</strong><div class="mt-2"><pre style="white-space: pre-wrap; background:#f8f9fa; padding:8px; border-radius:6px;">**Evidentiary Support and Citations** Use of data, analyses, studies, and references that substantiate claims and quantify effects.</pre></div></li>
<li><strong>Clarity_Structure_and_Procedural_Completeness_Rubric:</strong><div class="mt-2"><pre style="white-space: pre-wrap; background:#f8f9fa; padding:8px; border-radius:6px;">**Clarity Structure and Procedural Completeness** Organization, readability, civility, correct docket identifiers, accessible attachments, and contact details that facilitate agency action.</pre></div></li>
<li><strong>Specificity_of_Ask_and_Actionability_Rubric:</strong><div class="mt-2"><pre style="white-space: pre-wrap; background:#f8f9fa; padding:8px; border-radius:6px;">**Specificity of Ask and Actionability** Clarity of concrete requests or decisions sought from the agency, with steps the agency can realistically take.</pre></div></li>
<li><strong>Source_Credibility_and_Representation_Rubric:</strong><div class="mt-2"><pre style="white-space: pre-wrap; background:#f8f9fa; padding:8px; border-radius:6px;">**Source Credibility and Representation** Author expertise, organizational standing, coalition support, and whether the comment represents large or affected constituencies.</pre></div></li></ul></div></div>
            </div>
            <div class="accordion-item">
              <h2 class="accordion-header" id="usageHeader"><button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#usagePanel">Usage</button></h2>
              <div id="usagePanel" class="accordion-collapse collapse"><div class="accordion-body"><ul><li><strong>Evidentiary_Support_and_Citations_Rubric:</strong><div class="mt-2"><pre style="white-space: pre-wrap; background:#f8f9fa; padding:8px; border-radius:6px;">- **Domain:** Policy feedback triage / Evidence quality evaluation
- **Tasks:** 
  - Rank candidate drafts by strength of evidentiary support for escalation decisions
  - Detect vague, missing, or fabricated citations and flag traceability issues
  - Assess whether quantitative claims are sourced and whether magnitudes/timeframes are provided
  - Extract cited sources (titles, years, docket IDs, URLs) and summarize their relevance
  - Classify submissions into rubric categories (15) and provide brief rationale
  - Suggest concrete citation improvements or additional evidence needed for escalation
- **Best Suited For:** 
  - Large volumes of text-based policy feedback where fast, consistent triage is needed (e.g., initial pass to prioritize staff review).
  - Drafts that include explicit citation text (titles, authors, docket IDs, URLs) so the model can evaluate specificity and traceability.
  - Scenarios where the goal is to apply a fixed rubric across many items rather than perform final legal/technical adjudication.
  - Environments where the models output will be combined with human review or automated link-checking for verification.
  - When submissions make quantitative claims that can be judged for presence/absence of sources and unit/magnitude clarity.
  - Situations that benefit from consistent, descriptive feedback to contributors on how to improve evidence and citations.
- **Not Recommended For:** 
  - Needs real-time verification of external links, paywalled or subscription-only documents, or live data lookups that the model cannot perform.
  - Final agency decision-making or legal admissibility determinations that require authoritative primary-source validation.
  - Highly technical scientific or engineering claims that require reanalysis of primary datasets or replication of methods.
  - When submissions rely on unpublished/confidential documents or proprietary databases the model cannot access or verify.
  - Languages, formats, or domain-specific citation conventions the model has not been trained on or that are highly specialized.
  - Situations where hallucination risk is unacceptable and every citation must be independently confirmed without human oversight.</pre></div></li>
<li><strong>Clarity_Structure_and_Procedural_Completeness_Rubric:</strong><div class="mt-2"><pre style="white-space: pre-wrap; background:#f8f9fa; padding:8px; border-radius:6px;">- **Domain:** Document Triage / Text Evaluation
- **Tasks:** 
  - Triage public comments and citizen submissions for escalation to agency officials
  - Score and label drafts for clarity, organization, and civility against the provided rubric
  - Extract and validate presence/format of procedural identifiers (docket IDs, rule titles) from text
  - Detect and flag missing or incomplete contact information and produce a follow-up checklist
  - Assess attachments for accessibility when text-based content or searchable OCR is included
  - Generate short summary or executive-notes highlighting deficiencies that block agency action
  - Pre-screen submissions for potential procedural red flags (hostile tone, ambiguous docket references, inaccessible exhibits)
- **Best Suited For:** 
  - Input documents are provided as machine-readable text or searchable OCR (not image-only scans).
  - The rubric and expected docket ID formats are well-specified or follow consistent patterns the model can learn to recognize.
  - The goal is rapid triage and prioritization rather than final legal judgment.
  - Attachments are included inline or as clearly labeled text excerpts, enabling accessibility checks without external link fetching.
  - Human reviewers are available to act on items the model flags as high-priority or ambiguous.
- **Not Recommended For:** 
  - Attachments or evidence are only available via external links that the model cannot fetch or verify (broken links or remote files).
  - Submissions are image-only scans without OCR or have low-quality scans that impede text extraction.
  - Tasks require authoritative verification of docket numbers, filing status, or other facts that need access to agency databases or the internet.
  - High-stakes legal interpretation, official determinations, identity/authenticity verification, or decisions with regulatory liability are required without human oversight.
  - Multilingual inputs or domain-specific jargon without provided glossaries that might lead to tone and intent misclassification.</pre></div></li>
<li><strong>Specificity_of_Ask_and_Actionability_Rubric:</strong><div class="mt-2"><pre style="white-space: pre-wrap; background:#f8f9fa; padding:8px; border-radius:6px;">- **Domain:** Policy evaluation / Regulatory comment triage / Text classification
- **Tasks:** 
  - Triage and prioritization of public comments for agency escalation
  - Automated scoring of policy feedback drafts against a rubric (specificity/actionability)
  - Highlighting and extracting explicit asks, timelines, and responsible offices from submissions
  - Generating summary rationales for escalation decisions to assist human reviewers
  - Batch processing and ranking of candidate responses for docket-focused review
- **Best Suited For:** 
  - High-volume review contexts where consistent, rubric-based scoring is required to surface the most actionable submissions for human attention.
  - Submissions in English with reasonably clear prose and standard regulatory vocabulary (docket IDs, section references, statutory citations).
  - When the agency can provide the model with contextual metadata (docket number, relevant rule text, examples of scored comments) to calibrate judgments.
  - Triage workflows that require highlighting exact language supporting a score (e.g., explicit wording of asks, named offices, timelines) rather than final legal determinations.
  - Scenarios where the goal is to prioritize or summarize items for legal/policy staff who will conduct deeper review and decide on escalation.
- **Not Recommended For:** 
  - Cases that require definitive legal interpretation of statutes, delegation-of-authority questions, or binding legal advice without human legal review.
  - Highly technical scientific or engineering feasibility assessments that require domain specialists and empirical validation.
  - Comments in languages other than those the model was trained/tuned on, or submissions with heavy use of nonstandard abbreviations or poor OCR quality.
  - Situations with extremely low tolerance for hallucination (e.g., automatic redlining or issuing directives) where outputs would be acted on without human oversight.
  - Environments lacking representative training examples or contextual docket/rule information, which increases risks of inconsistent or inaccurate scoring.</pre></div></li>
<li><strong>Source_Credibility_and_Representation_Rubric:</strong><div class="mt-2"><pre style="white-space: pre-wrap; background:#f8f9fa; padding:8px; border-radius:6px;">- **Domain:** Policy feedback evaluation / Text classification
- **Tasks:** 
  - Triage of public comments for escalation to agency officials
  - Credibility scoring of policy feedback submissions
  - Identifying and enumerating signatories and coalition breadth in letters
  - Flagging potential astroturfing or mass-produced comments
  - Prioritizing high-impact stakeholder inputs for reviewer attention
  - Generating short rationales tied to rubric criteria for each score
  - Filtering submissions that require human verification or follow-up
- **Best Suited For:** 
  - Submissions primarily in English and in standard textual formats (letters, comments, emails) where identity, affiliation, and signatory information are present or omitted explicitly.
  - Large volumes of comments requiring consistent, repeatable triage according to a transparent rubric.
  - Situations where the goal is to prioritize which submissions merit human escalation rather than to conclusively verify external claims.
  - When comments include structured cues (letterhead, contact blocks, explicit membership counts, docket citations, lists of co-signers) that are directly mappable to the scoring guidelines.
  - Contexts that benefit from quick, explainable justifications for scores (e.g., to create human-review queues or summary dashboards).
- **Not Recommended For:** 
  - Cases that require external verification of claims (membership numbers, official roles, prior testimony) or authoritative fact-checking  the model cannot reliably confirm off-document facts.
  - Submissions that are multimedia-heavy (scanned images of letters, audio/video testimony) without accurate OCR/transcription, or texts in languages the model is weak in.
  - High-stakes legal or policy decisions where incorrect credibility assessments could cause material harm and thus require domain-expert adjudication.
  - Highly adversarial or purposely deceptive inputs designed to spoof organizational credentials or fabricate coalitions without corroborating evidence.
  - Narrow, highly technical domains with uncommon institutional codewords or localized organizational structures the model may not recognize without additional context.</pre></div></li></ul></div></div>
            </div>
            <div class="accordion-item">
              <h2 class="accordion-header" id="limitsHeader"><button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#limitsPanel">Limitations</button></h2>
              <div id="limitsPanel" class="accordion-collapse collapse"><div class="accordion-body"><ul><li><strong>Evidentiary_Support_and_Citations_Rubric:</strong><div class="mt-2"><pre style="white-space: pre-wrap; background:#f8f9fa; padding:8px; border-radius:6px;">- **Biases:** 
  - Authority bias: favoring government/academic sources and downgrading community, industry, or nontraditional evidence regardless of relevance.
  - Recency bias: giving more weight to newer sources even when older foundational studies or legal citations are more relevant.
  - Format bias: preferring fully formatted citations and URLs over informal but accurate references or attachments.
  - Language and geographic bias: privileging Englishlanguage or U.S. federal sources and undervaluing nonEnglish or local/regional evidence.
  - Quantitative bias: favoring submissions that present numbers even when qualitative evidence is more informative for the issue.
  - Traceability bias: equating the presence of a URL or docket ID with credibility without verifying content.
  - Publication bias: overvaluing peerreviewed literature and undervaluing credible gray literature (technical reports, agency memos).
  - Confirmation bias: tending to credit evidence that aligns with common assumptions about policy impacts more readily than surprising counterevidence.
  - Availability bias: relying on sources the model has been trained on or can recall easily, disadvantaging obscure but valid sources.
  - Conservatism bias: penalizing novel analyses or original calculations because they lack an established publication trail.
- **Task Misalignment Risks:** 
  - Scoring may prioritize traceable citation form over decision relevance, favoring citations that are easily verifiable rather than those most directly informing agency action.
  - The rubric can encourage citation paddingadding many superficial or lowquality references to get a higher scorerather than substantive evidence integration.
  - It may penalize concise but highquality submissions that summarize evidence without reproducing full citations or links.
  - The judge might undervalue firsthand community testimony or qualitative harms that are highly relevant for policy but lack traditional citations.
  - Because the model cannot access paywalled or proprietary attachments, it may downgrade otherwise strong, decisionrelevant evidence that isn't public.
  - The rubric assumes public, reproducible citations; submissions based on internal agency data or confidential sources could be misjudged.
  - Local or contextspecific evidence (e.g., tribal, municipal data) may be unfairly scored lower for not matching nationallevel citation norms.
  - The axis focuses on evidentiary depth and may miss timeliness or urgency signals that warrant escalation despite limited citations.
- **Failure Cases:** 
  - Hallucinated verification: the model asserts that quoted citations are valid or finds supporting text when it cannot actually access the source.
  - False negatives for shorthand citations: downgrading submissions that use accepted shorthand (e.g., 'EPA Toxics 2020') even though a human reviewer would recognize them.
  - False positives for fabricated citations: awarding higher scores to convincing but nonexistent reports or doctored docket IDs because format looks plausible.
  - Inability to follow links or attachments: incorrectly scoring a submission as lowevidence because links are to paywalled PDFs or attachments the model cannot retrieve.
  - Overrating quantity over quality: giving high scores to submissions listing many lowcredibility links (blogs, press releases) instead of a few strong sources.
  - Misinterpreting statistical claims: failing to spot misuse of statistics (e.g., incorrect denominators, conflating percentage points with percent change) and overrating evidence quality.
  - Failure to detect cherrypicking: not recognizing when numbers are selectively presented without acknowledging counterevidence or uncertainty.
  - Inconsistency across similar inputs: scoring similar submissions differently due to subtle wording changes that trigger different internal heuristics.
  - Context blindness: treating a legal citation as authoritative even when jurisdiction or statutory context makes it irrelevant to the agency's authority.
  - Adversarial formatting: being manipulated by maliciously formatted text (fake URLs, embedded citations in images) to inflate a submission's score.
  - Overpenalizing qualitative evidence: incorrectly assigning the lowest scores to persuasive firsthand accounts because they lack traditional citations.
  - Undervaluing original analysis: penalizing submissions that include novel calculations because the judge cannot reproduce methods or validate datasets.</pre></div></li>
<li><strong>Clarity_Structure_and_Procedural_Completeness_Rubric:</strong><div class="mt-2"><pre style="white-space: pre-wrap; background:#f8f9fa; padding:8px; border-radius:6px;">- **Biases:** 
  - Formality bias: preferring professional, well-formatted language and downranking grassroots or informal but actionable submissions.
  - Surface-cue bias: overweighting explicit metadata (docket numbers, headers) over implicit or paraphrased procedural references.
  - Native-language bias: misjudging clarity or civility for non-native or dialectal phrasing as less readable or less civil.
  - Civility detection errors: failing to detect sarcasm, coded hostility, or polite but hostile framing and thus mis-scoring tone.
  - Attachment accessibility bias: penalizing attachments that are accessible to humans but not machine-readable (e.g., scanned PDFs without OCR).
  - Conservatism/leniency calibration bias: systematic tendency to score more harshly or leniently depending on training examples or default thresholds.
  - Template familiarity bias: rewarding submissions that follow common templates and penalizing novel but valid organizational formats.
  - Confirmation/anchoring bias: initial cues (e.g., a clear docket ID) anchoring the score and causing the model to overlook other deficiencies.
- **Task Misalignment Risks:** 
  - Over-prioritizing formatting over substance, causing highly substantive but informally presented comments to be under-escalated.
  - Penalizing legitimate privacy-conscious submissions that intentionally omit full contact details, thereby missing actionable policy signals.
  - Treating presence of a docket ID as sufficient for escalation even when the substantive content references the wrong proceeding or misunderstanding.
  - Equating machine-readable attachments with real-world accessibility and ignoring context where a human reviewer could readily access materials.
  - Reducing complex procedural nuance (e.g., comments intended for different agency office or multiple dockets) to a single score, losing important distinctions for escalation.
  - Favoring submissions that match bureaucratic expectations and marginalizing nontraditional stakeholders (community groups, oral testimony transcripts).
  - Applying rigid civility standards that could suppress escalation of urgent but emotionally charged citizen feedback.
  - Failing to account for redacted or anonymized submissions that are nevertheless delegable to officials for policy significance.
- **Failure Cases:** 
  - False negative: a clearly actionable submission is scored low because it uses informal structure and lacks a formal header.
  - False positive: a well-formatted template letter with incorrect docket ID is scored highly and escalated despite pointing to the wrong rule.
  - OCR/attachment failure: attachments are present as scanned images and are treatable as inaccessible, causing downgrading despite being readable to human reviewers.
  - Link resolution failure: the model marks hyperlinks as broken because it cannot follow or validate them, even though they work in a browser.
  - Civility misclassification: polite but critical language is flagged as hostile due to sarcasm or idiomatic expressions, lowering the civility score.
  - Contact detection error: the model misses contact information that is present but embedded in a signature image or unconventional location.
  - Docket parsing error: transposed digits or slight formatting differences cause the model to treat a correct docket reference as absent/incorrect.
  - Inconsistent scoring: nearly identical submissions receive different scores across runs due to nondeterministic model outputs or subtle prompt sensitivity.
  - Over-reliance on explicit headings: submissions with clear logical flow but without explicit headings are underrated relative to wordier submissions with headings.
  - Accessibility overreach: the model downgrades submissions for not including alt text or accessibility metadata even when the agency does not require it, causing unnecessary non-escalation.
  - Privacy-preserving submission missed: anonymous or redacted submissions with high policy relevance are rejected because contact details are missing.
  - Hallucinated content: the model asserts the presence of attachments or contact details that do not exist, creating incorrect recommendations for escalation.</pre></div></li>
<li><strong>Specificity_of_Ask_and_Actionability_Rubric:</strong><div class="mt-2"><pre style="white-space: pre-wrap; background:#f8f9fa; padding:8px; border-radius:6px;">- **Biases:** 
  - Surface-form bias: overvaluing explicit citations, docket numbers, or legalistic phrasing even when underlying requests are impractical.
  - Recency and training-data bias: relying on patterns in training corpora that may not reflect current statutes, agency structures, or recent rulemaking practices.
  - Formality bias: favoring submissions that use technical or formal language and penalizing plain-language citizen input regardless of substance.
  - Jurisdictional bias: assuming common agency responsibilities or structures and misattributing which office/unit can act.
  - Conservatism bias: penalizing novel or unconventional but feasible implementation ideas because they deviate from familiar templates.
  - Confirmation bias from keywords: interpreting presence of certain words (e.g., 'CFR', 'docket', 'redline') as proxies for high actionability.
  - Language and cultural bias: lower scoring for submissions that use different rhetorical styles, idioms, or non-native-English phrasing.
  - Overconfidence bias: producing confident judgments about legal or operational feasibility despite lacking authoritative verification.
- **Task Misalignment Risks:** 
  - Keyword overfitting: optimizing for presence of axis-specific words (docket, CFR, redline) rather than true operational specificity, misaligning evaluation with real-world implementability.
  - Scope mismatch: treating any detailed request as actionable even when it requires funding, interagency coordination, or statutory change outside the agency's authority.
  - Overemphasis on formal legal markers: penalizing valid citizen asks that propose practical, program-level changes without formal citations.
  - Inflexible granularity: applying the same specificity standard to short public comments and to long policy memos, producing unfair comparisons.
  - Neglect of feasibility constraints: scoring high on 'specificity' without assessing whether the agency has the personnel, budget, or statutory authority to execute the plan.
  - Single-axis tunnel vision: ignoring other relevant evaluation axes (e.g., equity, evidence quality, legal risk), causing escalation of technically specific but harmful or legally invalid requests.
  - Misinterpretation of delegated asks: failing to detect when the citizen directs action to another entity (state, private sector), and thus mis-scoring agency-relevance.
  - Temporal mismatch: rewarding immediate-execution specificity when the agency's appropriate response is long-term rulemaking or research, not immediate operational changes.
- **Failure Cases:** 
  - False positive: a comment includes detailed-sounding redlines and office names but proposes actions that violate statute or require Congressional appropriation, yet receives a top score.
  - False negative: a succinct, plain-language submission that names a feasible procedural change (e.g., alter intake workflow) lacks citations and is scored low despite being immediately implementable.
  - Inconsistent calibration: near-duplicate comments with minor wording differences get different scores because the model latches onto differing surface tokens.
  - Gaming: submitters deliberately inject legalistic boilerplate (fake docket numbers, mock citations) to get higher escalation scores which the model cannot reliably verify.
  - Jurisdiction error: the model assigns responsibility to the wrong office/unit (e.g., regional enforcement vs. rulemaking staff) leading to improper escalation routing.
  - Context miss: the model fails to recognize that a requested metric or timeline is unrealistic given known rulemaking timelines and scores it as actionable.
  - Ambiguity failure: when a comment contains multiple asks with mixed levels of specificity, the model either averages out and obscures the actionable element or incorrectly focuses on the least actionable part.
  - Adversarial phrasing: politically charged or rhetorical language masks concrete asks that the model dismisses as opinion, missing otherwise actionable items.
  - Evidence-check failure: the model accepts citations or data claims at face value and rates an ask higher despite the underlying evidence being irrelevant or fabricated.
  - Cross-axis conflict: the model escalates a highly specific but legally risky proposal because it optimizes for this axis alone, ignoring legal risk that would disqualify escalation.</pre></div></li>
<li><strong>Source_Credibility_and_Representation_Rubric:</strong><div class="mt-2"><pre style="white-space: pre-wrap; background:#f8f9fa; padding:8px; border-radius:6px;">- **Biases:** 
  - Credentialism bias: privileging formal academic/professional credentials over lived experience or community knowledge.
  - Institutional/status bias: favoring large, well-known organizations and letterhead, disadvantaging small grassroots groups.
  - Format/visual bias: treating presence of letterhead, full contact info, or formal formatting as proxies for credibility.
  - Language and cultural bias: penalizing submissions from non-native English speakers or different rhetorical styles.
  - Name-recognition bias: giving extra weight to familiar institution names or senior-sounding titles.
  - Coalition-size bias: equating larger signatory lists with substantive representation without verifying diversity or authenticity.
  - Anonymity-averse bias: automatically lowering scores for anonymous submissions even when anonymity protects vulnerable individuals.
  - Surface-evidence bias: overrelying on explicit numeric claims (member counts, jurisdictions) that may be unverified or inflated.
  - Confirmation bias: interpreting ambiguous claims in a way that aligns with prior model exposure to similar documents.
  - Recency/training-data bias: preferring organizational types and presentation styles common in the model's training set.
- **Task Misalignment Risks:** 
  - Undervaluing affected individuals: the axis can systematically deprioritize individual but highly affected stakeholders who lack formal affiliation or credentials.
  - Overvaluing self-interested groups: well-resourced industry or advocacy organizations may be scored high despite narrow or biased interests.
  - Penalizing protected/vulnerable voices: anonymity or withheld contact info may reflect safety needs, not low credibility, leading to harmful exclusion.
  - Misreading coalition breadth: counting signatories as representation without assessing whether signers reflect diverse or relevant constituencies.
  - Conflating form with substance: the rubric may prioritize presentation quality (letterhead, citations) over factual accuracy or on-the-ground relevance.
  - Domain mismatch: a single rubric may fail to account for domain-specific credibility signals (e.g., patents in tech vs lived experience in community health).
  - False sense of verification: assigning high credibility based on claims the LLM cannot externally verify (membership numbers, prior testimony).
  - Disincentivizing grassroots testimony: public procedures may favor institutional commenters, skewing agency attention away from frontline impacts.
  - Ambiguity in multi-author submissions: the rubric may not specify how to score mixed-author inputs (expert co-signers plus anonymous community members).
  - Overemphasis on coalition breadth could ignore depth of impact (a small but uniquely affected group may be more relevant than a large but peripheral coalition).
- **Failure Cases:** 
  - Classifying an anonymous whistleblower's detailed, verifiable technical account as 'Very Low' because contact info is absent, thereby failing to escalate a critical issue.
  - Elevating a polished coalition letter with fabricated signatories and inflated membership counts to 'Very High' because of formatting and large numbers.
  - Downgrading grassroots submissions written in non-standard English or with culturally distinct rhetorical styles, missing important affectedness evidence.
  - Failing to detect astroturfing when a lobby group submits many coordinated comments with slight variations, instead scoring them as broad grassroots representation.
  - Scoring a small local nonprofit low because it lacks letterhead or formal membership numbers, despite the organization being the primary affected party.
  - Treating long, data-heavy industry comments with unverifiable proprietary claims as high credibility while ignoring conflicts of interest.
  - Missing domain nuance: giving a high score to an author with a PhD in an adjacent field whose expertise does not apply to the docket's technical area.
  - Misinterpreting co-signer lists: counting numerous minor endorsements as evidence of broad representation when signers are duplicate or irrelevant entities.
  - Penalizing a victim/survivor who omits contact info for privacy, thereby silencing a directly affected constituency in the escalation pipeline.
  - Failing to flag obviously fake organizations with realistic-sounding names because the model overrelies on lexical heuristics rather than verification.</pre></div></li></ul></div></div>
            </div>
          </div>
        </div>
      </div>

      <div class="col-md-6">
        <div class="card p-3 h-100">
          <h2>Compute Requirements</h2>
          <table class="table table-striped"><thead><tr><th>Metric</th><th>GPU RAM (MB)</th><th>CPU RAM (MB)</th></tr></thead>
            <tbody><tr><td>Evidentiary_Support_and_Citations_Rubric</td><td>--</td><td>--</td></tr><tr><td>Clarity_Structure_and_Procedural_Completeness_Rubric</td><td>--</td><td>--</td></tr><tr><td>Specificity_of_Ask_and_Actionability_Rubric</td><td>--</td><td>--</td></tr><tr><td>Source_Credibility_and_Representation_Rubric</td><td>--</td><td>--</td></tr></tbody>
          </table>
        </div>
      </div>
    </div>

    <div class="mt-5 card p-3">
      <h3>Metric Summary</h3>
      <p>This aggregate metric estimates how escalation-worthy a policy feedback draft is by linearly combining four rubric scores, with the greatest weight on evidentiary support and the next on clarity/procedural completeness. In practice, submissions that clearly cite sources, quantify claims, and are well-structured will be ranked higher, while precise asks/actionability contribute only marginal gains and source credibility currently has negligible effect. The intended use is fast, explainable triage: sort large volumes of comments, surface the top-scoring items for human review, and provide a consistent first-pass signal. Strengths include transparency (coefficients show which qualities matter), ease of maintenance, and alignment with review workflows that value traceable evidence and readable formats. Key limitations are the low sensitivity to actionability and the effective disregard for stakeholder credibility/representation, which can suppress escalation of highly feasible, constituency-significant input. Because the component rubrics carry known biases (e.g., favoring formal citations and large institutions), the aggregate score may systematically disadvantage grassroots or qualitative evidence unless counterbalanced. Treat the score as a ranking heuristic, not a probability; calibrate thresholds on a validation set and monitor precision/recall tradeoffs to meet capacity and equity goals. Refit and recalibrate periodically to avoid overfitting to historical patterns, and consider adding constraints or regularization if coefficient drift diminishes actionability or fairness. Pair automated ranking with targeted human checks for legal authority, verification of citations, and context on stakeholder representation.</p>
    </div>

    <div class="mt-4 card p-3">
      <div class="d-flex justify-content-between align-items-center mb-2">
        <h3 class="mb-0">Examples</h3>
        <button id="clear-examples-filter" class="btn btn-sm btn-outline-secondary" type="button">Show All</button>
      </div>
      
    </div>
  </div>

  <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/js/bootstrap.bundle.min.js"></script>
  <script src="https://cdn.datatables.net/2.0.8/js/dataTables.min.js"></script>
  <script>
    function getThemeLayout() {
      const color = getComputedStyle(document.body).color;
      return { paper_bgcolor: 'rgba(0,0,0,0)', plot_bgcolor: 'rgba(0,0,0,0)', font: { color } };
    }
    document.getElementById('darkModeToggle').addEventListener('change',e=>{document.body.classList.toggle('dark-mode',e.target.checked); drawAll();});
    // Enable tooltips
    const tooltipTriggerList = [].slice.call(document.querySelectorAll('[data-bs-toggle="tooltip"]'));
    tooltipTriggerList.map(function (el) {
      const tip = new bootstrap.Tooltip(el, {trigger: 'hover focus', delay: {show: 0, hide: 50}, placement: 'right'});
      el.addEventListener('shown.bs.tooltip', function () {
        try { if (window.MathJax && MathJax.typesetPromise) { MathJax.typesetPromise(); } } catch(_) {}
      });
      return tip;
    });

    // Initialize tooltips; use template content for robustness
    document.addEventListener('DOMContentLoaded', function () {
      document.querySelectorAll('.robust-tip').forEach(function (el) {
        const id = el.getAttribute('data-tip-id');
        let titleHtml = '';
        if (id) {
          const tpl = document.getElementById(id);
          if (tpl) titleHtml = tpl.innerHTML;
        }
        if (!titleHtml) {
          titleHtml = '<div style="max-width: 320px">Robustness tooltip unavailable.</div>';
        }
        const tip = new bootstrap.Tooltip(el, {
          trigger: 'hover focus',
          delay: {show: 0, hide: 50},
          placement: 'right',
          html: true,
          title: titleHtml
        });
      });
    });

    function drawCorrelation() {
      const layout = Object.assign({xaxis:{title:'Metric Score (normalized to target scale)'}, yaxis:{title:'Ground Truth'}}, getThemeLayout());
      layout.legend = layout.legend || {}; layout.legend.font = { size: 9 }; layout.margin = {l:40,r:10,t:30,b:40};
      const traces = [];
      if (RC_CORR.metrics) {
        // Determine top 3 metrics by absolute coefficient if available
        let topNames = [];
        try {
          const coeffPairs = ([["Evidentiary_Support_and_Citations_Rubric", 0.09914143738961213], ["Clarity_Structure_and_Procedural_Completeness_Rubric", 0.07288186958726793], ["Specificity_of_Ask_and_Actionability_Rubric", 0.005151409890197831], ["Source_Credibility_and_Representation_Rubric", -0.0], ["(intercept)", 0.505]]);
          const sorted = coeffPairs.filter(p=>p[0] !== '(intercept)').sort((a,b)=>Math.abs(b[1]) - Math.abs(a[1]));
          topNames = sorted.slice(0,3).map(p=>p[0]);
        } catch (e) { topNames = []; }
        for (const m of RC_CORR.metrics) {
          const rlab = (m.r!=null ? (m.r.toFixed ? m.r.toFixed(2) : m.r) : 'NA');
          const tlab = (m.tau!=null ? (m.tau.toFixed ? m.tau.toFixed(2) : m.tau) : 'NA');
          const visible = (topNames.includes(m.name)) ? true : 'legendonly';
          const ids = m.ids || [];
          const text = ids.map(id => 'ID: ' + id);
          traces.push({ x: m.x_norm || m.x || [], y: m.y || [], mode: 'markers', name: (m.name || '') + ' (r=' + rlab + ', =' + tlab + ')', visible, text: text, hovertemplate: '%{text}<br>x=%{x:.3f}<br>y=%{y:.3f}<extra></extra>' });
        }
      }
      if (RC_CORR.regression) {
        const rlab = (RC_CORR.regression.r!=null ? (RC_CORR.regression.r.toFixed ? RC_CORR.regression.r.toFixed(2) : RC_CORR.regression.r) : 'NA');
        const tlab = (RC_CORR.regression.tau!=null ? (RC_CORR.regression.tau.toFixed ? RC_CORR.regression.tau.toFixed(2) : RC_CORR.regression.tau) : 'NA');
        const ids = RC_CORR.regression.ids || [];
        const text = ids.map(id => 'ID: ' + id);
        traces.push({ x: RC_CORR.regression.x_norm || RC_CORR.regression.x || [], y: RC_CORR.regression.y || [], mode: 'markers', name: (RC_CORR.regression.name || 'Aggregate') + ' (r=' + rlab + ', =' + tlab + ')', marker: { size: 8, color: 'black' }, text: text, hovertemplate: '%{text}<br>x=%{x:.3f}<br>y=%{y:.3f}<extra></extra>' });
        document.getElementById('correlation-stats').innerText = 'Aggregate metric: r=' + rlab + ', =' + tlab;
      }
      Plotly.newPlot('correlation-chart', traces, layout, {displayModeBar: false});
      // Click-to-jump: when a point is clicked, locate its ID in the examples table and jump to it
      const chart = document.getElementById('correlation-chart');
      chart.on('plotly_click', function(data) {
        try {
          if (!data || !data.points || data.points.length === 0) return;
          const pt = data.points[0];
          const idText = (pt.text || '').toString(); // format: 'ID: <val>'
          const id = idText.startsWith('ID: ') ? idText.slice(4) : idText;
          const tblEl = document.getElementById('examples-table');
          if (!tblEl) return;
          // Try DataTables jQuery API first
          if (window.jQuery && jQuery.fn && jQuery.fn.dataTable) {
            const dt = jQuery(tblEl).DataTable();
            // Search by exact match in first column (ID)
            dt.search('');
            dt.columns(0).search('^' + id.replace(/[.*+?^${}()|[\]\\]/g, '\\$&') + '$', true, false).draw();
            // Scroll into view first visible row after draw completes
            setTimeout(function(){
              let rowNode = null;
              try {
                const idxs = dt.rows({ search: 'applied' }).indexes();
                if (idxs && idxs.length) rowNode = dt.row(idxs[0]).node();
              } catch(_){ }
              if (!rowNode) {
                try { rowNode = dt.row(0).node(); } catch(_) {}
              }
              if (rowNode && rowNode.scrollIntoView) {
                rowNode.scrollIntoView({behavior:'smooth', block:'center'});
                try { rowNode.classList.add('table-active'); setTimeout(()=>rowNode.classList.remove('table-active'), 1200); } catch(_) {}
              }
            }, 60);
          } else if (typeof DataTable !== 'undefined') {
            // Vanilla DataTables 2 API
            const dt = DataTable.get(tblEl) || new DataTable(tblEl);
            dt.search('');
            // Filter to rows whose first cell (ID) matches
            dt.columns().every(function(idx) {
              if (idx === 0) {
                this.search('^' + id.replace(/[.*+?^${}()|[\]\\]/g, '\\$&') + '$', true, false);
              } else {
                this.search('');
              }
            });
            dt.draw();
            setTimeout(function(){
              let firstRow = null;
              try {
                const nodes = dt.rows({ search: 'applied' }).nodes();
                if (nodes && nodes.length) firstRow = nodes[0];
              } catch(_) {}
              if (!firstRow) {
                const body = tblEl.tBodies && tblEl.tBodies[0];
                firstRow = body && body.rows && body.rows[0];
              }
              if (!firstRow) {
                try {
                  const rows = Array.from(tblEl.tBodies[0].rows || []);
                  firstRow = rows.find(r => (r.cells && r.cells[0] && (r.cells[0].textContent||'').trim() === id));
                } catch(_) {}
              }
              if (firstRow && firstRow.scrollIntoView) {
                firstRow.scrollIntoView({behavior:'smooth', block:'center'});
                try { firstRow.classList.add('table-active'); setTimeout(()=>firstRow.classList.remove('table-active'), 1200); } catch(_) {}
              }
            }, 60);
          }
        } catch(e) { try { console.error('[ReportCard] click-jump failed', e); } catch(_){} }
      });
    }

    function drawRuntime() {
      const layout = Object.assign({yaxis:{title:'Time per Sample (s)'}}, getThemeLayout());
      const boxes = [];
      if (RC_RUNTIME.per_metric) {
        for (const [name, arr] of Object.entries(RC_RUNTIME.per_metric)) {
          boxes.push({ y: arr, type: 'box', name });
        }
      }
      Plotly.newPlot('runtime-chart', boxes, layout);
      if (RC_RUNTIME.aggregate) {
        const agg = RC_RUNTIME.aggregate;
        var seq = (agg.sequence_mean||0);
        if (typeof seq === 'number' && seq.toFixed) { seq = seq.toFixed(2); }
        var par = (agg.parallel_mean||0);
        if (typeof par === 'number' && par.toFixed) { par = par.toFixed(2); }
        var seqCI = (agg.sequence_ci||0);
        if (typeof seqCI === 'number' && seqCI.toFixed) { seqCI = seqCI.toFixed(2); }
        var parCI = (agg.parallel_ci||0);
        if (typeof parCI === 'number' && parCI.toFixed) { parCI = parCI.toFixed(2); }
        document.getElementById('runtime-info').innerHTML = 'Avg time/sample (sequence): ' + seq + 's  ' + seqCI + 's' + '<br/>' + 'Avg time/sample (parallel): ' + par + 's  ' + parCI + 's (95% CI)';
      }
    }

    function drawRobustness() {
      if (!RC_ROB.available || !RC_ROB.scores) {
        document.getElementById('robustness-sens').innerHTML = '<em>Robustness not available.</em>';
        document.getElementById('robustness-stab').innerHTML = '';
        return;
      }
      const names = Object.keys(RC_ROB.scores);
      const sens = names.map(n => (RC_ROB.scores[n] && RC_ROB.scores[n].sensitivity) || 0);
      const stab = names.map(n => (RC_ROB.scores[n] && RC_ROB.scores[n].stability) || 0);
      Plotly.newPlot('robustness-sens', [{x: names, y: sens, type:'bar', name:'Sensitivity'}], Object.assign({yaxis:{title:'Sensitivity'}}, getThemeLayout()));
      Plotly.newPlot('robustness-stab', [{x: names, y: stab, type:'bar', name:'Stability'}], Object.assign({yaxis:{title:'Stability'}}, getThemeLayout()));
    }

    function downloadPython() {
      try {
        const code = RC_PY_CODE || '';
        if (!code) { return; }
        const blob = new Blob([code], { type: 'text/x-python' });
        const url = URL.createObjectURL(blob);
        const a = document.createElement('a');
        a.href = url;
        const name = (RC_PY_FILENAME && typeof RC_PY_FILENAME === 'string' && RC_PY_FILENAME.trim()) ? RC_PY_FILENAME : 'AutoMetricsRegression.py';
        a.download = name;
        document.body.appendChild(a);
        a.click();
        setTimeout(function(){ URL.revokeObjectURL(url); try { a.remove(); } catch(_){} }, 0);
      } catch(_) { }
    }

    function drawAll() { drawCorrelation(); drawRuntime(); drawRobustness(); }
    drawAll();
  </script>
  <!-- Modal for Metric Card -->
  <div class="modal fade" id="metricDocModal" tabindex="-1" aria-hidden="true">
    <div class="modal-dialog modal-xl modal-dialog-scrollable">
      <div class="modal-content">
        <div class="modal-header">
          <h5 class="modal-title" id="metricDocTitle"></h5>
          <button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button>
        </div>
        <div class="modal-body">
          <div id="metricDocBody" style="white-space: normal;"></div>
        </div>
      </div>
    </div>
  </div>
  <script>
    (function() {
      const tbl = document.getElementById('examples-table');
      if (!tbl) return;
      const clearBtn = document.getElementById('clear-examples-filter');
      try {
        if (window.jQuery && jQuery.fn && typeof jQuery.fn.dataTable !== 'undefined') {
          jQuery(tbl).DataTable({
            paging: true,
            pageLength: 5,
            ordering: true,
            searching: true,
            scrollX: true
          });
          if (clearBtn) {
            clearBtn.addEventListener('click', function(){
              try {
                const dt = jQuery(tbl).DataTable();
                dt.search('');
                dt.columns().every(function(){ this.search(''); });
                dt.draw();
              } catch(_) {}
            });
          }
        } else if (typeof DataTable !== 'undefined') {
          new DataTable(tbl, {
            paging: true,
            pageLength: 5,
            ordering: true,
            searching: true,
            scrollX: true
          });
          if (clearBtn) {
            clearBtn.addEventListener('click', function(){
              try {
                const dt = DataTable.get(tbl);
                dt.search('');
                dt.columns().every(function(){ this.search(''); });
                dt.draw();
              } catch(_) {}
            });
          }
        }
      } catch (e) { try { console.error('[ReportCard] DataTables init error:', e); } catch(_){} }
    })();
  </script>
  <script>
    // Click handlers for regression coefficient metric links -> open modal with metric card
    document.addEventListener('click', function(e) {
      const a = e.target.closest && e.target.closest('a.coeff-link');
      if (!a) return;
      e.preventDefault();
      try {
        let metric = a.getAttribute('data-metric');
        // Resolve submetric to parent metric if available
        if (RC_DOCS && !(metric in RC_DOCS) && RC_DOCS_MAP && RC_DOCS_MAP[metric]) {
          metric = RC_DOCS_MAP[metric];
        }
        const doc = (RC_DOCS && RC_DOCS[metric]) ? RC_DOCS[metric] : 'No metric card available.';
        const titleNode = document.getElementById('metricDocTitle');
        const bodyNode = document.getElementById('metricDocBody');
        if (titleNode) titleNode.textContent = metric + '  Metric Card';
        if (bodyNode) {
          try {
            bodyNode.innerHTML = marked.parse(doc);
          } catch(_) {
            bodyNode.textContent = doc;
          }
        }
        const modalEl = document.getElementById('metricDocModal');
        if (modalEl && bootstrap && bootstrap.Modal) {
          const modal = bootstrap.Modal.getOrCreateInstance(modalEl, {backdrop: true});
          modal.show();
        }
      } catch(_) {}
    });
  </script>
  <div id="robustness-tip-template" class="d-none">
    <div style="max-width: 360px">
      <strong>Sensitivity</strong> (worse_obvious): how much the metric tends to drop when the output is intentionally degraded. For each example, we measure the relative drop from the original to the average worse_obvious score, clip negative values to 0 (no drop), and then average across examples.
      <br/><br/>
      <strong>Stability</strong> (same_obvious): how consistent the metric stays under neutral edits that should not change meaning. For each example, we measure how close the original is to the average same_obvious score (scaled by the original magnitude), clip below 0, and then average across examples. Higher means more stable.
    </div>
  </div>
</body>
</html>
