## Overview

## Comparisons

* automatically-generated rubrics vs. rubrics with a weak reward vs. evolving rubrics (reward function changes over time based on data being collected, X)
* articulable vs. inarticulable vs. noise — how do metrics evolve through the training process?
* metric scaling laws — how many (a) datapoints are needed to generate a good metric and (b) how many metrics are needed to improve the quality of the agentic training process?

## running thoughts 
* should the scaffolding metric generator start with a weakly supervised metric? Or just start with a basic LLM judge metric?

## Thoughts on Cognitive Framing:

Most contemporary evaluation work in NLP and RLHF treats metrics as fixed: BLEU-like overlaps, pretrained reward models, or hand-written rubrics that are assumed to faithfully stand in for human judgment. Recent work on **AutoMetrics** pushes against this by learning evaluation metrics from small amounts of task-specific feedback, composing a set of base scores (classic metrics plus LLM-as-a-judge criteria) into a single scalar that correlates well with human labels and can even be used as a proxy reward for optimization. But AutoMetrics still behaves like a one-shot regression: generate many candidate metrics, run a partial least squares fit, keep the top few, and stop. It does not explicitly reason about *what* parts of human judgment these metrics capture, what remains uncaptured, or how this balance varies across domains and individuals.

Our discussion reframed this as a **measurement modeling** problem: there is a latent “taste” or “quality” function underlying human evaluations, and we want to decompose it into interpretable components, tacit components, and noise. Drawing on Polanyi’s notion of *tacit knowledge* and Ryle’s “knowing how” vs “knowing that,” we distinguished between aspects of judgment that can be expressed as short, human-usable rules (“articulable”), aspects that show up systematically in behavior but resist such compression (“inarticulable but modelable”), and irreducible randomness. Bourdieu’s work on taste and habitus suggests these components will differ across social fields (e.g., regulators vs newsrooms), while Adorno and aesthetic theory highlight the possibility that some evaluative content may always exceed explicit concepts. We imported this into a modeling lens by defining articulability *relative to a function class and description budget*: what can be captured by a small collection of named metrics and rubrics (LLM-as-judge executing explicit rules) versus what requires a high-capacity critic (large prompt, finetuned reward model) versus what remains unexplained.

On top of AutoMetrics, we sketched a next-generation framework—call it **AutoMetrics++**—that makes this decomposition explicit. First, **sequential metric development**: instead of generating all metrics once, we fit an initial composite metric to human labels, examine residuals, and then deliberately generate new LLM-judge metrics to explain remaining variance (“what systematic patterns is the current metric blind to?”). Each round yields new axes or refined rubrics, whose length, structural complexity, and stability we track over time. This lets us empirically study whether later metrics become more baroque and niche—an indicator that we are pushing the limits of articulability in that domain. Second, optionally, **iterative refinement** of individual metrics: treating each metric like a concept in a concept-bottleneck model, we can edit its rubric or examples to better align with human labels while regularizing for simplicity, watching which concepts “snap into place” versus which only improve via ever-longer, more conditional rules. Finally, after exhausting this interpretable layer, we fit a **black-box residual model** (e.g., a reward model on embeddings or an optimized LLM judge) to the remaining residuals, explicitly marking this as the “inarticulable but modelable” component, and treating the final unexplained variance as effective noise or dependence on unobserved factors.

This decomposition suggests concrete, testable hypotheses about **institutional versus aesthetic domains**. In bureaucratic settings like public-comment triage, agencies often operate under semi-formal frameworks for what must be addressed (statutory relevance, novelty of legal argument, evidence, coalition size). We would expect a relatively large share of variance in “response salience” to be captured by a small number of interpretable metrics derived from those criteria, with a modest residual taste head. In more aesthetic or editorial settings like newsworthiness decisions, by contrast, organizations may invoke high-level values (“impact,” “public interest”) but rely heavily on tacit newsroom judgment about timing, narrative, and audience appeal. There we might see interpretable metrics saturate quickly, with a much larger black-box component absorbing stylistic and contextual taste—and a bigger truly unpredictable remainder. This is closely related to Scott’s “legibility vs metis” story: in some systems, pushing decisions through legible metrics will preserve performance; in others, over-metricization risks stripping away valuable tacit know-how.

We also explored a **personalization lens**, where the same decomposition is applied across multiple raters: outlets, agencies, survey respondents, or legislators. A global interpretable metric provides a shared scaffold, while user-specific deviations—either sparse adjustments to metric weights or low-rank latent factors—capture individualized taste. This lets us ask questions like: which evaluation dimensions are *common* across actors (low variability in metric weights) and which are *highly personalized*? What is the minimal number of “taste dimensions” needed to adapt a global model to an individual (a personalization rank or sparsity measure)? In policy-adjacent data, mass survey projects like CES/ANES give respondent×issue matrices of policy preferences, while roll-call data from Voteview gives legislator×bill matrices of yea/nay votes structured by party and ideology. Factoring out shared ideological dimensions (e.g., via ideal-point models) and applying AutoMetrics++ to residuals would let us quantify how much elite or mass policy taste is explainable by a handful of explicit axes versus idiosyncratic or tacit structure.

Finally, we outlined an experimental program using the policy-centric datasets already available: (1) **newsworthiness judgments** on city-council policies across multiple local newspapers, and (2) **public-comment response salience** across agencies. For each, we would build an initial AutoMetrics-style pool of LLM-judge metrics (clarity, local impact, legal relevance, etc.), run sequential residual-driven metric discovery and optional iterative rubric refinement, and then fit a black-box residual critic. This design supports several empirical analyses: decomposing variance into articulable vs black-box vs noise across domains; examining how newly added metrics grow in length and complexity over rounds, identifying “switch points” where further rubric elaboration yields diminishing returns; and tracing **scaling curves** showing how many labeled examples are needed to learn stable, high-performing interpretable metrics in institutional vs aesthetic settings. Together, these pieces turn AutoMetrics from a one-shot meta-evaluator into a tool for probing the structure of human judgment itself, quantifying where and how far we can push explicit, inspectable evaluation criteria before we must rely on opaque learned critics and residual taste.
