{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nlp/scr/mryan0/miniconda3/envs/autometrics/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<class 'autometrics.metrics.reference_based.BLEU.BLEU'>, <class 'autometrics.metrics.reference_based.CHRF.CHRF'>, <class 'autometrics.metrics.reference_based.TER.TER'>, <class 'autometrics.metrics.reference_based.GLEU.GLEU'>, <class 'autometrics.metrics.reference_based.SARI.SARI'>, <class 'autometrics.metrics.reference_based.BERTScore.BERTScore'>, <class 'autometrics.metrics.reference_based.ROUGE.ROUGE'>, <class 'autometrics.metrics.reference_based.MOVERScore.MOVERScore'>, <class 'autometrics.metrics.reference_based.BARTScore.BARTScore'>, <class 'autometrics.metrics.reference_based.UniEvalDialogue.UniEvalDialogue'>, <class 'autometrics.metrics.reference_based.UniEvalSum.UniEvalSum'>, <class 'autometrics.metrics.reference_based.CIDEr.CIDEr'>, <class 'autometrics.metrics.reference_based.METEOR.METEOR'>, <class 'autometrics.metrics.reference_based.BLEURT.BLEURT'>, <class 'autometrics.metrics.reference_based.StringSimilarity.LevenshteinDistance'>, <class 'autometrics.metrics.reference_based.StringSimilarity.LevenshteinRatio'>, <class 'autometrics.metrics.reference_based.StringSimilarity.HammingDistance'>, <class 'autometrics.metrics.reference_based.StringSimilarity.JaroSimilarity'>, <class 'autometrics.metrics.reference_based.StringSimilarity.JaroWinklerSimilarity'>, <class 'autometrics.metrics.reference_based.StringSimilarity.JaccardDistance'>, <class 'autometrics.metrics.reference_based.ParaScore.ParaScore'>, <class 'autometrics.metrics.reference_based.YiSi.YiSi'>, <class 'autometrics.metrics.reference_based.MAUVE.MAUVE'>, <class 'autometrics.metrics.reference_based.PseudoPARENT.PseudoPARENT'>, <class 'autometrics.metrics.reference_based.NIST.NIST'>, <class 'autometrics.metrics.reference_based.IBLEU.IBLEU'>, <class 'autometrics.metrics.reference_based.UpdateROUGE.UpdateROUGE'>, <class 'autometrics.metrics.reference_based.LENS.LENSMetric'>, <class 'autometrics.metrics.reference_based.CharCut.CharCut'>, <class 'autometrics.metrics.reference_based.InfoLM.InfoLM'>, <class 'autometrics.metrics.reference_free.FKGL.FKGL'>, <class 'autometrics.metrics.reference_free.UniEvalFact.UniEvalFact'>, <class 'autometrics.metrics.reference_free.Perplexity.Perplexity'>, <class 'autometrics.metrics.reference_free.ParaScoreFree.ParaScoreFree'>, <class 'autometrics.metrics.reference_free.INFORMRewardModel.INFORMRewardModel'>, <class 'autometrics.metrics.reference_free.PRMRewardModel.MathProcessRewardModel'>, <class 'autometrics.metrics.reference_free.SummaQA.SummaQA'>, <class 'autometrics.metrics.reference_free.DistinctNGram.DistinctNGram'>, <class 'autometrics.metrics.reference_free.FastTextToxicity.FastTextToxicity'>, <class 'autometrics.metrics.reference_free.FastTextNSFW.FastTextNSFW'>, <class 'autometrics.metrics.reference_free.FastTextEducationalValue.FastTextEducationalValue'>, <class 'autometrics.metrics.reference_free.SelfBLEU.SelfBLEU'>, <class 'autometrics.metrics.reference_free.FactCC.FactCC'>, <class 'autometrics.metrics.reference_free.Toxicity.Toxicity'>, <class 'autometrics.metrics.reference_free.Sentiment.Sentiment'>, <class 'autometrics.metrics.reference_free.GRMRewardModel.GRMRewardModel'>, <class 'autometrics.metrics.reference_free.LENS_SALSA.LENS_SALSA'>, <class 'autometrics.metrics.reference_free.LDLRewardModel.LDLRewardModel'>]\n"
     ]
    }
   ],
   "source": [
    "from autometrics.metrics.MetricBank import all_metric_classes\n",
    "\n",
    "print(all_metric_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "# Metric Card for BLEU\n",
      "\n",
      "BLEU (Bilingual Evaluation Understudy) is a widely used metric for evaluating the quality of text generated in tasks like machine translation and summarization. It measures the overlap of n-grams between a generated text and one or more reference texts, with a brevity penalty to penalize overly short translations. SacreBLEU, a modern implementation, ensures reproducibility and standardization of BLEU scores across research.\n",
      "\n",
      "## Metric Details\n",
      "\n",
      "### Metric Description\n",
      "\n",
      "BLEU evaluates the quality of text generation by comparing n-grams in the generated output with those in one or more reference texts. It computes modified precision for n-grams and combines scores using a geometric mean, with a brevity penalty to ensure the length of the generated text matches that of the references. Higher BLEU scores indicate closer similarity to the references.\n",
      "\n",
      "- **Metric Type:** Surface-Level Similarity\n",
      "- **Range:** 0 to 1\n",
      "- **Higher is Better?:** Yes\n",
      "- **Reference-Based?:** Yes\n",
      "- **Input-Required?:** No\n",
      "\n",
      "### Formal Definition\n",
      "\n",
      "$$\n",
      "ight)BLEU} = \text{BP} \times \\exp \\left( \\sum_{n=1}^N w_n \\log p_n \n",
      "$$\n",
      "\n",
      "where:\n",
      "- $\text{BP} = \\min(1, e^{1 - r/c})$ is the brevity penalty,\n",
      "- $r$ is the effective reference length (based on the closest matching reference length for each sentence),\n",
      "- $c$ is the candidate translation length,\n",
      "- $p_n$ is the modified precision for n-grams of length $n$,\n",
      "- $w_n$ are weights for each n-gram (commonly uniform, $w_n = \frac{1}{N}$).\n",
      "\n",
      "### Inputs and Outputs\n",
      "\n",
      "- **Inputs:**  \n",
      "- Generated text (candidate translation)  \n",
      "- Reference text(s) (gold-standard translations)  \n",
      "\n",
      "- **Outputs:**  \n",
      "- Scalar BLEU score (range: 0 to 1)\n",
      "\n",
      "## Intended Use\n",
      "\n",
      "### Domains and Tasks\n",
      "\n",
      "- **Domain:** Text Generation\n",
      "- **Tasks:** Machine Translation, Summarization, Data-to-Text Generation\n",
      "\n",
      "### Applicability and Limitations\n",
      "\n",
      "- **Best Suited For:**  \n",
      "Structured tasks with a clear correspondence between generated and reference texts, such as translation or summarization.\n",
      "\n",
      "- **Not Recommended For:**  \n",
      "Open-ended or creative generation tasks where diversity or semantic similarity matters more than lexical overlap (e.g., storytelling, dialogue).\n",
      "\n",
      "## Metric Implementation\n",
      "\n",
      "### Reference Implementations\n",
      "\n",
      "- **Libraries/Packages:**\n",
      "- [SacreBLEU](https://github.com/mjpost/sacrebleu) (robust, standard implementation)\n",
      "- [NLTK](https://www.nltk.org/api/nltk.translate.html) (basic Python implementation)\n",
      "- [Hugging Face `evaluate`](https://huggingface.co/docs/evaluate) (integrated metric framework)\n",
      "\n",
      "### Computational Complexity\n",
      "\n",
      "- **Efficiency:**  \n",
      "BLEU is computationally efficient, requiring $O(n \times m)$ operations for $n$-gram matching where $n$ is the number of words in the candidate text and $m$ is the number of reference words. SacreBLEU optimizes tokenization and scoring, making it highly suitable for large-scale evaluations.\n",
      "\n",
      "- **Scalability:**  \n",
      "BLEU scales well across datasets of varying sizes due to its simple design. SacreBLEU further supports evaluation with multiple references, diverse tokenization schemes, and language-specific preprocessing, making it adaptable to diverse evaluation setups.\n",
      "\n",
      "## Known Limitations\n",
      "\n",
      "- **Biases:**  \n",
      "- BLEU penalizes valid paraphrases or semantically equivalent outputs that do not match reference n-grams exactly.  \n",
      "- The brevity penalty can overly penalize valid shorter outputs, particularly for tasks where shorter text may be acceptable or even preferred (e.g., summarization).  \n",
      "\n",
      "- **Task Misalignment Risks:**  \n",
      "- BLEU is not designed for evaluating tasks with high diversity in acceptable outputs (e.g., open-ended dialogue).  \n",
      "- Scores depend on the quality and number of references; fewer or inconsistent references can lead to misleading evaluations.\n",
      "\n",
      "- **Failure Cases:**  \n",
      "- BLEU struggles to capture semantic adequacy beyond lexical similarity. For instance, it cannot identify whether a translation preserves the meaning of the original sentence if word choices diverge significantly.\n",
      "\n",
      "## Related Metrics\n",
      "\n",
      "- **ROUGE:** Often used for summarization tasks, emphasizing recall over precision.  \n",
      "- **METEOR:** Incorporates synonym matching for better semantic alignment.  \n",
      "- **BERTScore:** Uses contextual embeddings for semantic similarity.  \n",
      "\n",
      "## Further Reading\n",
      "\n",
      "- **Papers:**  \n",
      "- [Original BLEU Paper (Papineni et al., 2002)](https://www.aclweb.org/anthology/P02-1040)  \n",
      "- [SacreBLEU: A Call for Clarity in Reporting BLEU Scores (Post, 2018)](https://www.aclweb.org/anthology/W18-6319)\n",
      "\n",
      "- **Blogs/Tutorials:**  \n",
      "- [Understanding BLEU](https://machinelearningmastery.com/calculate-bleu-score-for-text-python/)  \n",
      "- [SacreBLEU Documentation](https://github.com/mjpost/sacrebleu)\n",
      "\n",
      "## Citation\n",
      "\n",
      "```\n",
      "@inproceedings{papineni-etal-2002-bleu,\n",
      "    title = \"{B}leu: a Method for Automatic Evaluation of Machine Translation\",\n",
      "    author = \"Papineni, Kishore  and\n",
      "      Roukos, Salim  and\n",
      "      Ward, Todd  and\n",
      "      Zhu, Wei-Jing\",\n",
      "    editor = \"Isabelle, Pierre  and\n",
      "      Charniak, Eugene  and\n",
      "      Lin, Dekang\",\n",
      "    booktitle = \"Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics\",\n",
      "    month = jul,\n",
      "    year = \"2002\",\n",
      "    address = \"Philadelphia, Pennsylvania, USA\",\n",
      "    publisher = \"Association for Computational Linguistics\",\n",
      "    url = \"https://aclanthology.org/P02-1040/\",\n",
      "    doi = \"10.3115/1073083.1073135\",\n",
      "    pages = \"311--318\"\n",
      "}\n",
      "```\n",
      "\n",
      "## Metric Card Authors\n",
      "\n",
      "- **Authors:** Michael J. Ryan  \n",
      "- **Acknowledgment of AI Assistance:**  \n",
      "Portions of this metric card were drafted with assistance from OpenAI's ChatGPT, based on user-provided inputs and relevant documentation. All content has been reviewed and curated by the author to ensure accuracy.  \n",
      "- **Contact:** mryan0@stanford.edu\n",
      "    \n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(all_metric_classes[0].__doc__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autometrics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
